\begin{abstract}
	Navigation and mapping on the lunar surface require robust perception under challenging conditions, including poorly textured environments, high-contrast lighting, and limited computational resources. This paper presents a real-time mapping framework that integrates dense perception models with a 3D Gaussian Splatting (3DGS) representation. We first benchmark a suite of models on the synthetic LuSNAR dataset, selecting RAFT-Stereo for its balance of speed and accuracy in depth estimation, and U-Net++ for its superior performance in detecting hazardous rocks. Using ground truth poses, our pipeline reconstructs a 500-meter traverse with a geometric height accuracy of approximately 10 cm, achieving a quality comparable to a traditional point cloud baseline. The resulting 3DGS map enables high-fidelity novel view synthesis and serves as a foundation for a full SLAM system, where its capacity for joint map and pose optimization would offer significant advantages. Our results demonstrate that combining dense perception with explicit neural representations is an effective approach for creating detailed, large-scale maps to support future lunar rover missions.
\end{abstract}
\vspace{-3em}