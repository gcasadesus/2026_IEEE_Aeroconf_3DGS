\section{Conclusions}
\label{sec:conclusion}
In this work, we presented and evaluated a real-time framework for creating dense, semantic 3D maps of the lunar surface. Our approach integrates dense perception networks with a 3D Gaussian Splatting (3DGS) representation to address the unique challenges of lunar environments. The primary contributions are the benchmarking of modern perception models for this domain and the demonstration of their integration into an incremental, high-fidelity mapping pipeline.

Our experimental evaluation on the LuSNAR dataset provided several key findings. We identified RAFT-Stereo as a suitable depth estimation model, offering a robust balance between accuracy and real-time processing speed. For semantic segmentation, U-Net++ was selected for its superior performance in detecting rocks, a critical capability for hazard avoidance; its effectiveness was confirmed by the fact that using ground truth segmentation provided no quantitative improvement to the final map. When using these models, our pipeline achieved a geometric height accuracy of approximately 10 cm. Under the assumption of perfect poses, the reconstruction quality was comparable to a traditional point cloud baseline, with the largest errors occurring in distant, poorly-lit craters and along the dark edges of rocks where perception models are most likely to fail.

This work establishes a foundation for several avenues of future research. The most critical next step is to integrate a tracking front-end to create a complete Simultaneous Localization and Mapping (SLAM) system. This will remove the reliance on ground truth poses and test the 3DGS representation's ability to jointly refine the map and camera trajectory, which we hypothesize will show a significant advantage over simpler aggregation methods. Future work will also focus on more sophisticated, density-based techniques for surface extraction from the Gaussians to better capture the continuous geometry. Finally, a more rigorous characterization of the system—including its performance with varied camera configurations, additional datasets, and the incorporation of perception uncertainty—is necessary to develop advanced, map-aware path planning algorithms that can directly enhance rover autonomy.