\section{Introduction}

\subsection{Motivation}
The renewed focus on robotic exploration of the lunar surface brings new challenges for autonomous navigation and mapping~\cite{euroconsult_prospects_2023}. Long-range traverses are required in poorly textured environments, and high-contrast and dynamic lighting conditions, as shown in \cref{fig:sample_apollo}. Furthermore, rovers are often limited to radiation-hardened hardware and low-power sensors, priorizing cameras over higher-resolution alternatives like LiDAR. These constraints require the development of computationally efficient, vision-based mapping and localization solutions.

Simultaneous Localization and Mapping (SLAM) is a fundamental technique for an agent to construct a map of an unknown environment while concurrently tracking its position within that map. Classical SLAM methods, such as those based on geometric features~\cite{campos_orb-slam3_2021,newcombe_kinectfusion_2011}, can operate in real time but exhibit reduced performance in poorly textured and high-contrast lighting conditions. While learning-based SLAM methods can improve robustness by learning feature representations from data, their generalization to novel environments like the lunar surface is not guaranteed. A common limitation of both classical and early learning-based methods is their reliance on discrete map representations (e.g., point clouds, voxels), which can be memory-intensive and may not capture fine surface detail.

\begin{figure}[b]
	\vspace{-1em}
	\centering
	\begin{subfigure}[b]{0.55\linewidth}
		\includegraphics[width=\linewidth,trim=0 0.5 0.5em 0,clip]{figures/rover2.png}
	\end{subfigure}
	\begin{subfigure}[b]{0.395\linewidth}
		\includegraphics[width=\linewidth,trim=0 0.5 0.5em 0,clip]{figures/surface_astronaut.png}
	\end{subfigure}
	\caption{\bfseries Images from the Apollo missions~\cite{nasa_apollo_2017}.}
	\label{fig:sample_apollo}
\end{figure}

Recent advances in neural scene representations have produced methods capable of high-fidelity 3D reconstruction from images. For terrestrial applications, recent work has proposed a variety of approaches using different sensor inputs (e.g., monocular, stereo, depth, IMU) and scene encodings such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS)~\cite{tosi_how_2024}. These systems often rely on perception models for tasks like feature extraction, depth estimation, or semantic understanding. However, since these perception components are typically trained on terrestrial data, it is not clear whether their performance will generalize to the unique visual characteristics of extraterrestrial environments.

\subsection{Related Work}

Semantic characterization of extraterrestrial scenes is an active area of research. A systematic literature review on semantic terrain segmentation for planetary rovers~\cite{kuang_semantic_2022} highlights a gap in existing solutions, noting that no current method simultaneously achieves pixel-level accuracy, real-time inference, and compatibility with onboard hardware. Specific models have shown promise for feature identification, such as detecting impact craters from digital elevation models~\cite{jia_moon_2021}. Despite these advances in perception, the integration of such semantic information into modern neural mapping frameworks for planetary surfaces has not been thoroughly investigated.

Several works have applied neural scene representations to model the lunar surface. Some have focused on surface reconstruction from orbital imagery to generate digital elevation models, often to handle challenging illumination in permanently shadowed regions~\cite{van_kints_neural_2025, adams_summary_2023}. More relevant to rover navigation, a few recent studies have explored using NeRFs with surface-level imagery for localization, mapping, and path planning~\cite{huang_monocular_2025, hansen_analyzing_2024, dai_neural_2023, zhang_neural_2024}.

While these approaches demonstrate the potential of neural representations for capturing lunar terrain, they mostly rely on NeRF~\cite{mildenhall_nerf_2021}, which has several limitations for real-time mapping. First, NeRF's rendering process is computationally expensive due to its reliance on volumetric sampling, making it ill-suited for real-time applications on resource-constrained hardware. Second, the underlying multi-layer perceptron (MLP) represents a fixed volume, which is difficult to extend as the rover explores new areas and is not easily deformable to accommodate loop closures. This necessitates stitching multiple models, which can introduce inconsistencies. Finally, the implicit nature of NeRF makes it less interpretable and difficult to integrate with or query for explicit semantic information. Many of these works also assume diverse camera viewpoints during training, a condition not met by typical rover trajectories.

\subsection{Contributions}
This work addresses the aforementioned limitations by developing a framework for building semantic, large-scale maps of the lunar surface in real time. Our contributions are:
\begin{itemize}
	\item We propose a real-time mapping framework for lunar surface environments based on 3DGS, which offers fast rendering and a flexible, explicit scene representation suitable for incremental updates.
	\item We integrate and benchmark a several perception models, including semantic segmentation and both stereo and monocular depth estimation networks. We analyze the direct impact of these perception inputs on the geometric and semantic quality of the final 3D reconstruction.
\end{itemize}


\subsection{Paper Organization}
The rest of the paper is organized as follows. \Cref{sec:datasets_and_models} describes the selected dataset and details the perception models used for monocular depth estimation, stereo depth estimation, and semantic segmentation, as well as the fundamentals of 3D Gaussian Splatting. \Cref{sec:methodology} presents our proposed real-time mapping framework, outlining the perception front-end, the incremental mapping process, and the optimization back-end, including the loss function. \Cref{sec:results} defines the evaluation metrics and presents a quantitative and qualitative analysis of the dense depth estimation, semantic segmentation, and final surface reconstruction results. Finally, \Cref{sec:conclusion} concludes the paper with a summary of our findings and a discussion of future work.

% \begin{figure*}[t]
% 	\centering
% 	% \begin{subfigure}[t]{0.33\linewidth}
% 	%   \includegraphics[width=\linewidth]{figures/sample_open3d.pdf}
% 	%   \caption{\bfseries Open3D environment.}
% 	%   \label{fig:sample_open3d}
% 	% \end{subfigure}
% 	\begin{subfigure}[t]{0.33\linewidth}
% 		\includegraphics[width=\linewidth]{figures/sample_lac.pdf}
% 		\caption{\bfseries LAC environment.}\label{fig:sample_lac}
% 	\end{subfigure}
% 	\begin{subfigure}[t]{0.33\linewidth}
% 		\includegraphics[width=\linewidth]{figures/sample_lusnar.pdf}
% 		\caption{\bfseries LuSNAR dataset.}\label{fig:sample_lusnar}
% 	\end{subfigure}
% 	\caption{\bfseries Sample images from different environments.}\label{fig:sample_environments}
% \end{figure*}

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=\linewidth]{figures/sample_lusnar.pdf}
% 	\caption{\bfseries Sample images from the LuSNAR dataset.}
% 	\label{fig:sample_lusnar}
% \end{figure}
