@ieeetranbstctl{IEEEexample:BSTcontrol,
  ctluse_url = {no}
}

@article{kiamouche_performance_2011,
  title        = {Performance Analysis of a Dynamic Channel Reservation-Like Technique for Low Earth Orbit Mobile Satellite Systems},
  volume       = {5},
  url          = {https://publications.waset.org/1096/performance-analysis-of-a-dynamic-channel-reservation-like-technique-for-low-earth-orbit-mobile-satellite-systems},
  abstract     = {Performance Analysis of a Dynamic Channel Reservation-Like Technique for Low Earth Orbit Mobile Satellite Systems},
  pages        = {471--476},
  number       = {3},
  journaltitle = {International Journal of Electronics and Communication Engineering},
  author       = {Kiamouche, W. and Lasmari, S. and Benslama, M.},
  urldate      = {2022-07-24},
  date         = {2011-03-26},
  langid       = {english},
  note         = {tex.ids= Kiamoucheb},
  keywords     = {notion, Gemini},
  file         = {Citeseer - Snapshot:/Users/guillemcv/AppData/Zotero/storage/M5FF2LK4/summary.html:text/html;Kiamouche et al_2011_Performance Analysis of a Dynamic Channel Reservation-Like Technique for Low.pdf:/Users/guillemcv/Documents (Offline)/Zotero/Kiamouche et al_2011_Performance Analysis of a Dynamic Channel Reservation-Like Technique for Low.pdf:application/pdf;Kiamouche et al_2011_Performance Analysis of a Dynamic Channel Reservation-Like Technique for Low.pdf:/Users/guillemcv/Documents (Offline)/Zotero/Kiamouche et al_2011_Performance Analysis of a Dynamic Channel Reservation-Like Technique for Low2.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/7PL9QXP6/performance-analysis-of-a-dynamic-channel-reservation-like-technique-for-low-earth-orbit-mobile.html:text/html}
}

@article{islam_comparative_2020,
  title        = {A comparative study of similarity-based and {GNN}-based link prediction approaches},
  url          = {http://arxiv.org/abs/2008.08879},
  abstract     = {The task of inferring the missing links in a graph based on its current structure is referred to as link prediction. Link prediction methods that are based on pairwise node similarity are well-established approaches in the literature. They show good prediction performance in many real-world graphs though they are heuristics and lack of universal applicability. On the other hand, the success of neural networks for classiﬁcation tasks in various domains leads researchers to study them in graphs. When a neural network can operate directly on the graph, then it is termed as the graph neural network ({GNN}). {GNN} is able to learn hidden features from graphs which can be used for link prediction task in graphs. Link predictions based on {GNNs} have gained much attention of researchers due to their convincing high performance in many real-world graphs. This appraisal paper studies some similarity and {GNN}-based link prediction approaches in the domain of homogeneous graphs that consists of a single type of (attributed)nodes and single type of pairwise links. We evaluate the studied approaches against several benchmark graphs with diﬀerent properties from various domains.},
  journaltitle = {{arXiv}:2008.08879 [cs]},
  author       = {Islam, Md Kamrul and Aridhi, Sabeur and Smail-Tabbone, Malika},
  urldate      = {2021-04-11},
  date         = {2020-08-20},
  langid       = {english},
  eprinttype   = {arxiv},
  eprint       = {2008.08879},
  keywords     = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
  file         = {Islam et al_2020_A comparative study of similarity-based and GNN-based link prediction approaches.pdf:/Users/guillemcv/Documents (Offline)/Zotero/Islam et al_2020_A comparative study of similarity-based and GNN-based link prediction approaches.pdf:application/pdf}
}

@inproceedings{dor_visual_2021,
  location   = {Nashville, {TN}, {USA}},
  title      = {Visual {SLAM} for Asteroid Relative Navigation},
  isbn       = {978-1-6654-4899-4},
  url        = {https://ieeexplore.ieee.org/document/9522783/},
  doi        = {10.1109/CVPRW53098.2021.00235},
  abstract   = {This paper focuses on the application of visual {SLAM} for the purpose of precise autonomous navigation around an asteroid. We develop a factor graph-based approach allowing for incremental growth and fusion of sensor orientation measurements, Earth-relative inertial position measurements, as well as in-situ monocular camera imagerybased measurements, with an emphasis on the initialization step. Crucially, and in contrast to typical simulated scenarios found in the literature, we validate our approach using real imagery from {NASA}’s {DAWN} mission to asteroid Vesta, along with navigation comparison data from the {NASA} {NAIF} {SPICE} kernels. Quantitative comparisons show impressive accuracy for a typical target characterization phase segment, both in terms of the estimated trajectory as well as in terms of the tracked estimated landmarks. Based on these results, this paper further supports the viability of autonomous {SLAM}-based navigation for deepspace asteroid missions.},
  eventtitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
  pages      = {2066--2075},
  booktitle  = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
  publisher  = {{IEEE}},
  author     = {Dor, Mehregan and Skinner, Katherine A. and Tsiotras, Panagiotis and Driver, Travis},
  urldate    = {2023-07-05},
  date       = {2021-06},
  langid     = {english},
  note       = {6 citations (Crossref) [2023-11-10]},
  file       = {Dor et al_2021_Visual SLAM for Asteroid Relative Navigation.pdf:/Users/guillemcv/Documents (Offline)/Zotero/Dor et al_2021_Visual SLAM for Asteroid Relative Navigation.pdf:application/pdf}
}

@article{mur-artal_orb-slam2_2017,
  title        = {{ORB}-{SLAM}2: An Open-Source {SLAM} System for Monocular, Stereo, and {RGB}-D Cameras},
  volume       = {33},
  issn         = {1941-0468},
  url          = {https://ieeexplore.ieee.org/document/7946260},
  doi          = {10.1109/TRO.2017.2705103},
  shorttitle   = {{ORB}-{SLAM}2},
  abstract     = {We present {ORB}-{SLAM}2, a complete simultaneous localization and mapping ({SLAM}) system for monocular, stereo and {RGB}-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate {SLAM} solution. We publish the source code, not only for the benefit of the {SLAM} community, but with the aim of being an out-of-the-box {SLAM} solution for researchers in other fields.},
  pages        = {1255--1262},
  number       = {5},
  journaltitle = {{IEEE} Transactions on Robotics},
  author       = {Mur-Artal, Raúl and Tardós, Juan D.},
  urldate      = {2024-10-19},
  date         = {2017-10},
  note         = {{IEEE} Transactions on Robotics},
  keywords     = {Optimization, Feature extraction, Trajectory, Cameras, Localization, mapping, {RGB}-D, Simultaneous localization and mapping, simultaneous localization and mapping ({SLAM}), stereo, Tracking loops},
  file         = {IEEE Xplore Abstract Record:/Users/guillemcv/AppData/Zotero/storage/JFKAYXI5/7946260.html:text/html;Submitted Version:/Users/guillemcv/AppData/Zotero/storage/FW5DXFE5/Mur-Artal and Tardós - 2017 - ORB-SLAM2 An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras.pdf:application/pdf}
}

@article{mildenhall_nerf_2021,
  title        = {{NeRF}: representing scenes as neural radiance fields for view synthesis},
  volume       = {65},
  issn         = {0001-0782},
  url          = {https://dl.acm.org/doi/10.1145/3503250},
  doi          = {10.1145/3503250},
  shorttitle   = {{NeRF}},
  abstract     = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully connected (nonconvolutional) deep network, whose input is a single continuous 5D coordinate (spatial location (x, y, z) and viewing direction (θ, ϕ)) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis.},
  pages        = {99--106},
  number       = {1},
  journaltitle = {Commun. {ACM}},
  author       = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  urldate      = {2024-10-19},
  date         = {2021-12-17},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/SR4D78YV/Mildenhall et al. - 2021 - NeRF representing scenes as neural radiance fields for view synthesis.pdf:application/pdf}
}

@inproceedings{whelan_elasticfusion_2015,
  title      = {{ElasticFusion}: Dense {SLAM} Without A Pose Graph},
  isbn       = {978-0-9923747-1-6},
  url        = {http://www.roboticsproceedings.org/rss11/p01.pdf},
  doi        = {10.15607/RSS.2015.XI.001},
  shorttitle = {{ElasticFusion}},
  abstract   = {We present a novel approach to real-time dense visual {SLAM}. Our system is capable of capturing comprehensive dense globally consistent surfel-based maps of room scale environments explored using an {RGB}-D camera in an incremental online fashion, without pose graph optimisation or any postprocessing steps. This is accomplished by using dense frame-tomodel camera tracking and windowed surfel-based fusion coupled with frequent model reﬁnement through non-rigid surface deformations. Our approach applies local model-to-model surface loop closure optimisations as often as possible to stay close to the mode of the map distribution, while utilising global loop closure to recover from arbitrary drift and maintain global consistency.},
  eventtitle = {Robotics: Science and Systems 2015},
  booktitle  = {Robotics: Science and Systems {XI}},
  publisher  = {Robotics: Science and Systems Foundation},
  author     = {Whelan, Thomas and Leutenegger, Stefan and Salas Moreno, Renato and Glocker, Ben and Davison, Andrew},
  urldate    = {2024-10-19},
  date       = {2015-07-13},
  langid     = {english},
  file       = {PDF:/Users/guillemcv/AppData/Zotero/storage/76IHK789/Whelan et al. - 2015 - ElasticFusion Dense SLAM Without A Pose Graph.pdf:application/pdf}
}

@article{kerbl_3d_2023,
  title        = {3D Gaussian Splatting for Real-Time Radiance Field Rendering.},
  volume       = {42},
  pages        = {139--1},
  number       = {4},
  journaltitle = {{ACM} Trans. Graph.},
  author       = {Kerbl, Bernhard and Kopanas, Georgios and Leimkühler, Thomas and Drettakis, George},
  date         = {2023},
  file         = {PDF:/Users/guillemcv/AppData/Zotero/storage/W725HVEN/Kerbl et al. - 2023 - 3D Gaussian Splatting for Real-Time Radiance Field Rendering.pdf:application/pdf}
}

@inproceedings{keetha_splatam_2024,
  location   = {Seattle, {WA}, {USA}},
  title      = {{SplaTAM}: Splat, Track \& Map 3D Gaussians for Dense {RGB}-D {SLAM}},
  rights     = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-5300-6},
  url        = {https://ieeexplore.ieee.org/document/10656349/},
  doi        = {10.1109/CVPR52733.2024.02018},
  shorttitle = {{SplaTAM}},
  abstract   = {Dense simultaneous localization and mapping ({SLAM}) is crucial for robotics and augmented reality applications. However, current methods are often hampered by the nonvolumetric or implicit way they represent a scene. This work introduces {SplaTAM}, an approach that, for the first time, leverages explicit volumetric representations, i.e., 3D Gaussians, to enable high-fidelity reconstruction from a single unposed {RGB}-D camera, surpassing the capabilities of existing methods. {SplaTAM} employs a simple online tracking and mapping system tailored to the underlying Gaussian representation. It utilizes a silhouette mask to elegantly capture the presence of scene density. This combination enables several benefits over prior representations, including fast rendering and dense optimization, quickly determining if areas have been previously mapped, and structured map expansion by adding more Gaussians. Extensive experiments show that {SplaTAM} achieves up to 2× superior performance in camera pose estimation, map construction, and novel-view synthesis over existing methods, paving the way for more immersive high-fidelity {SLAM} applications.},
  eventtitle = {2024 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages      = {21357--21366},
  booktitle  = {2024 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher  = {{IEEE}},
  author     = {Keetha, Nikhil and Karhade, Jay and Jatavallabhula, Krishna Murthy and Yang, Gengshan and Scherer, Sebastian and Ramanan, Deva and Luiten, Jonathon},
  urldate    = {2024-11-06},
  date       = {2024-06-16},
  langid     = {english},
  file       = {PDF:/Users/guillemcv/AppData/Zotero/storage/9BAAW8MF/Keetha et al. - 2024 - SplaTAM Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM.pdf:application/pdf}
}

@misc{yugay_gaussian-slam_2024,
  title      = {Gaussian-{SLAM}: Photo-realistic Dense {SLAM} with Gaussian Splatting},
  url        = {http://arxiv.org/abs/2312.10070},
  doi        = {10.48550/arXiv.2312.10070},
  shorttitle = {Gaussian-{SLAM}},
  abstract   = {We present a dense simultaneous localization and mapping ({SLAM}) method that uses 3D Gaussians as a scene representation. Our approach enables interactive-time reconstruction and photo-realistic rendering from real-world single-camera {RGBD} videos. To this end, we propose a novel effective strategy for seeding new Gaussians for newly explored areas and their effective online optimization that is independent of the scene size and thus scalable to larger scenes. This is achieved by organizing the scene into sub-maps which are independently optimized and do not need to be kept in memory. We further accomplish frame-to-model camera tracking by minimizing photometric and geometric losses between the input and rendered frames. The Gaussian representation allows for high-quality photo-realistic real-time rendering of real-world scenes. Evaluation on synthetic and real-world datasets demonstrates competitive or superior performance in mapping, tracking, and rendering compared to existing neural dense {SLAM} methods.},
  number     = {{arXiv}:2312.10070},
  publisher  = {{arXiv}},
  author     = {Yugay, Vladimir and Li, Yue and Gevers, Theo and Oswald, Martin R.},
  urldate    = {2024-11-06},
  date       = {2024-03-22},
  eprinttype = {arxiv},
  eprint     = {2312.10070},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/3I4KUNB7/Yugay et al. - 2024 - Gaussian-SLAM Photo-realistic Dense SLAM with Gaussian Splatting.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/GS468KC2/2312.html:text/html}
}

@misc{tosi_how_2024,
  title      = {How {NeRFs} and 3D Gaussian Splatting are Reshaping {SLAM}: a Survey},
  url        = {http://arxiv.org/abs/2402.13255},
  doi        = {10.48550/arXiv.2402.13255},
  shorttitle = {How {NeRFs} and 3D Gaussian Splatting are Reshaping {SLAM}},
  abstract   = {Over the past two decades, research in the field of Simultaneous Localization and Mapping ({SLAM}) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields ({NeRFs}) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of {SLAM} progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.},
  number     = {{arXiv}:2402.13255},
  publisher  = {{arXiv}},
  author     = {Tosi, Fabio and Zhang, Youmin and Gong, Ziren and Sandström, Erik and Mattoccia, Stefano and Oswald, Martin R. and Poggi, Matteo},
  urldate    = {2025-01-23},
  date       = {2024-04-11},
  eprinttype = {arxiv},
  eprint     = {2402.13255 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/ICT54FGZ/Tosi et al. - 2024 - How NeRFs and 3D Gaussian Splatting are Reshaping SLAM a Survey.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/WRCCP87A/2402.html:text/html}
}

@misc{zhang_glorie-slam_2024,
  title      = {{GlORIE}-{SLAM}: Globally Optimized {RGB}-only Implicit Encoding Point Cloud {SLAM}},
  url        = {http://arxiv.org/abs/2403.19549},
  doi        = {10.48550/arXiv.2403.19549},
  shorttitle = {{GlORIE}-{SLAM}},
  abstract   = {Recent advancements in {RGB}-only dense Simultaneous Localization and Mapping ({SLAM}) have predominantly utilized grid-based neural implicit encodings and/or struggle to efficiently realize global map and pose consistency. To this end, we propose an efficient {RGB}-only dense {SLAM} system using a flexible neural point cloud scene representation that adapts to keyframe poses and depth updates, without needing costly backpropagation. Another critical challenge of {RGB}-only {SLAM} is the lack of geometric priors. To alleviate this issue, with the aid of a monocular depth estimator, we introduce a novel {DSPO} layer for bundle adjustment which optimizes the pose and depth of keyframes along with the scale of the monocular depth. Finally, our system benefits from loop closure and online global bundle adjustment and performs either better or competitive to existing dense neural {RGB} {SLAM} methods in tracking, mapping and rendering accuracy on the Replica, {TUM}-{RGBD} and {ScanNet} datasets. The source code is available at https://github.com/zhangganlin/{GlOIRE}-{SLAM}},
  number     = {{arXiv}:2403.19549},
  publisher  = {{arXiv}},
  author     = {Zhang, Ganlin and Sandström, Erik and Zhang, Youmin and Patel, Manthan and Gool, Luc Van and Oswald, Martin R.},
  urldate    = {2025-01-24},
  date       = {2024-05-27},
  eprinttype = {arxiv},
  eprint     = {2403.19549 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/J5AT3M3N/Zhang et al. - 2024 - GlORIE-SLAM Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/3MU2NAA2/2403.html:text/html}
}

@misc{zhou_mod-slam_2024,
  title      = {{MoD}-{SLAM}: Monocular Dense Mapping for Unbounded 3D Scene Reconstruction},
  url        = {http://arxiv.org/abs/2402.03762},
  doi        = {10.48550/arXiv.2402.03762},
  shorttitle = {{MoD}-{SLAM}},
  abstract   = {Monocular {SLAM} has received a lot of attention due to its simple {RGB} inputs and the lifting of complex sensor constraints. However, existing monocular {SLAM} systems are designed for bounded scenes, restricting the applicability of {SLAM} systems. To address this limitation, we propose {MoD}-{SLAM}, the first monocular {NeRF}-based dense mapping method that allows 3D reconstruction in real-time in unbounded scenes. Specifically, we introduce a Gaussian-based unbounded scene representation approach to solve the challenge of mapping scenes without boundaries. This strategy is essential to extend the {SLAM} application. Moreover, a depth estimation module in the front-end is designed to extract accurate priori depth values to supervise mapping and tracking processes. By introducing a robust depth loss term into the tracking process, our {SLAM} system achieves more precise pose estimation in large-scale scenes. Our experiments on two standard datasets show that {MoD}-{SLAM} achieves competitive performance, improving the accuracy of the 3D reconstruction and localization by up to 30\% and 15\% respectively compared with existing state-of-the-art monocular {SLAM} systems.},
  number     = {{arXiv}:2402.03762},
  publisher  = {{arXiv}},
  author     = {Zhou, Heng and Guo, Zhetao and Liu, Shuhong and Zhang, Lechen and Wang, Qihao and Ren, Yuxiang and Li, Mingrui},
  urldate    = {2025-01-24},
  date       = {2024-03-08},
  eprinttype = {arxiv},
  eprint     = {2402.03762 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/CXRUAI2F/Zhou et al. - 2024 - MoD-SLAM Monocular Dense Mapping for Unbounded 3D Scene Reconstruction.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/RG489RTI/2402.html:text/html}
}

@misc{zhang_go-slam_2023,
  title      = {{GO}-{SLAM}: Global Optimization for Consistent 3D Instant Reconstruction},
  url        = {http://arxiv.org/abs/2309.02436},
  doi        = {10.48550/arXiv.2309.02436},
  shorttitle = {{GO}-{SLAM}},
  abstract   = {Neural implicit representations have recently demonstrated compelling results on dense Simultaneous Localization And Mapping ({SLAM}) but suffer from the accumulation of errors in camera tracking and distortion in the reconstruction. Purposely, we present {GO}-{SLAM}, a deep-learning-based dense visual {SLAM} framework globally optimizing poses and 3D reconstruction in real-time. Robust pose estimation is at its core, supported by efficient loop closing and online full bundle adjustment, which optimize per frame by utilizing the learned global geometry of the complete history of input frames. Simultaneously, we update the implicit and continuous surface representation on-the-fly to ensure global consistency of 3D reconstruction. Results on various synthetic and real-world datasets demonstrate that {GO}-{SLAM} outperforms state-of-the-art approaches at tracking robustness and reconstruction accuracy. Furthermore, {GO}-{SLAM} is versatile and can run with monocular, stereo, and {RGB}-D input.},
  number     = {{arXiv}:2309.02436},
  publisher  = {{arXiv}},
  author     = {Zhang, Youmin and Tosi, Fabio and Mattoccia, Stefano and Poggi, Matteo},
  urldate    = {2025-01-24},
  date       = {2023-09-05},
  eprinttype = {arxiv},
  eprint     = {2309.02436 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/KNZATIR2/Zhang et al. - 2023 - GO-SLAM Global Optimization for Consistent 3D Instant Reconstruction.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/LEEFWWBZ/2309.html:text/html}
}

@misc{rosinol_probabilistic_2022,
  title      = {Probabilistic Volumetric Fusion for Dense Monocular {SLAM}},
  url        = {http://arxiv.org/abs/2210.01276},
  doi        = {10.48550/arXiv.2210.01276},
  abstract   = {We present a novel method to reconstruct 3D scenes from images by leveraging deep dense monocular {SLAM} and fast uncertainty propagation. The proposed approach is able to 3D reconstruct scenes densely, accurately, and in real-time while being robust to extremely noisy depth estimates coming from dense monocular {SLAM}. Differently from previous approaches, that either use ad-hoc depth filters, or that estimate the depth uncertainty from {RGB}-D cameras' sensor models, our probabilistic depth uncertainty derives directly from the information matrix of the underlying bundle adjustment problem in {SLAM}. We show that the resulting depth uncertainty provides an excellent signal to weight the depth-maps for volumetric fusion. Without our depth uncertainty, the resulting mesh is noisy and with artifacts, while our approach generates an accurate 3D mesh with significantly fewer artifacts. We provide results on the challenging Euroc dataset, and show that our approach achieves 92\% better accuracy than directly fusing depths from monocular {SLAM}, and up to 90\% improvements compared to the best competing approach.},
  number     = {{arXiv}:2210.01276},
  publisher  = {{arXiv}},
  author     = {Rosinol, Antoni and Leonard, John J. and Carlone, Luca},
  urldate    = {2025-01-24},
  date       = {2022-10-16},
  eprinttype = {arxiv},
  eprint     = {2210.01276 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/LF277IDL/Rosinol et al. - 2022 - Probabilistic Volumetric Fusion for Dense Monocular SLAM.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/8B4MJ2JX/2210.html:text/html}
}

@misc{rosinol_nerf-slam_2022,
  title      = {{NeRF}-{SLAM}: Real-Time Dense Monocular {SLAM} with Neural Radiance Fields},
  url        = {http://arxiv.org/abs/2210.13641},
  doi        = {10.48550/arXiv.2210.13641},
  shorttitle = {{NeRF}-{SLAM}},
  abstract   = {We propose a novel geometric and photometric 3D mapping pipeline for accurate and real-time scene reconstruction from monocular images. To achieve this, we leverage recent advances in dense monocular {SLAM} and real-time hierarchical volumetric neural radiance fields. Our insight is that dense monocular {SLAM} provides the right information to fit a neural radiance field of the scene in real-time, by providing accurate pose estimates and depth-maps with associated uncertainty. With our proposed uncertainty-based depth loss, we achieve not only good photometric accuracy, but also great geometric accuracy. In fact, our proposed pipeline achieves better geometric and photometric accuracy than competing approaches (up to 179\% better {PSNR} and 86\% better L1 depth), while working in real-time and using only monocular images.},
  number     = {{arXiv}:2210.13641},
  publisher  = {{arXiv}},
  author     = {Rosinol, Antoni and Leonard, John J. and Carlone, Luca},
  urldate    = {2025-01-24},
  date       = {2022-10-24},
  eprinttype = {arxiv},
  eprint     = {2210.13641 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/5YMLKW6L/Rosinol et al. - 2022 - NeRF-SLAM Real-Time Dense Monocular SLAM with Neural Radiance Fields.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/TCWBAWKJ/2210.html:text/html}
}

@misc{yan_mute-slam_2024,
  title      = {{MUTE}-{SLAM}: Real-Time Neural {SLAM} with Multiple Tri-Plane Hash Representations},
  url        = {http://arxiv.org/abs/2403.17765},
  doi        = {10.48550/arXiv.2403.17765},
  shorttitle = {{MUTE}-{SLAM}},
  abstract   = {We introduce {MUTE}-{SLAM}, a real-time neural {RGB}-D {SLAM} system employing multiple tri-plane hash-encodings for efficient scene representation. {MUTE}-{SLAM} effectively tracks camera positions and incrementally builds a scalable multi-map representation for both small and large indoor environments. As previous methods often require pre-defined scene boundaries, {MUTE}-{SLAM} dynamically allocates sub-maps for newly observed local regions, enabling constraint-free mapping without prior scene information. Unlike traditional grid-based methods, we use three orthogonal axis-aligned planes for hash-encoding scene properties, significantly reducing hash collisions and the number of trainable parameters. This hybrid approach not only ensures real-time performance but also enhances the fidelity of surface reconstruction. Furthermore, our optimization strategy concurrently optimizes all sub-maps intersecting with the current camera frustum, ensuring global consistency. Extensive testing on both real-world and synthetic datasets has shown that {MUTE}-{SLAM} delivers state-of-the-art surface reconstruction quality and competitive tracking performance across diverse indoor settings. The code is available at https://github.com/{lumennYan}/{MUTE}\_SLAM.},
  number     = {{arXiv}:2403.17765},
  publisher  = {{arXiv}},
  author     = {Yan, Yifan and He, Ruomin and Liu, Zhenghua},
  urldate    = {2025-01-24},
  date       = {2024-09-21},
  eprinttype = {arxiv},
  eprint     = {2403.17765 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/2XCGTWUK/Yan et al. - 2024 - MUTE-SLAM Real-Time Neural SLAM with Multiple Tri-Plane Hash Representations.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/26I8IH95/2403.html:text/html}
}

@misc{liso_loopy-slam_2024,
  title      = {Loopy-{SLAM}: Dense Neural {SLAM} with Loop Closures},
  url        = {http://arxiv.org/abs/2402.09944},
  doi        = {10.48550/arXiv.2402.09944},
  shorttitle = {Loopy-{SLAM}},
  abstract   = {Neural {RGBD} {SLAM} techniques have shown promise in dense Simultaneous Localization And Mapping ({SLAM}), yet face challenges such as error accumulation during camera tracking resulting in distorted maps. In response, we introduce Loopy-{SLAM} that globally optimizes poses and the dense 3D model. We use frame-to-model tracking using a data-driven point-based submap generation method and trigger loop closures online by performing global place recognition. Robust pose graph optimization is used to rigidly align the local submaps. As our representation is point based, map corrections can be performed efficiently without the need to store the entire history of input frames used for mapping as typically required by methods employing a grid based mapping structure. Evaluation on the synthetic Replica and real-world {TUM}-{RGBD} and {ScanNet} datasets demonstrate competitive or superior performance in tracking, mapping, and rendering accuracy when compared to existing dense neural {RGBD} {SLAM} methods. Project page: notchla.github.io/Loopy-{SLAM}.},
  number     = {{arXiv}:2402.09944},
  publisher  = {{arXiv}},
  author     = {Liso, Lorenzo and Sandström, Erik and Yugay, Vladimir and Gool, Luc Van and Oswald, Martin R.},
  urldate    = {2025-01-24},
  date       = {2024-06-10},
  eprinttype = {arxiv},
  eprint     = {2402.09944 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/DB4GQ6V7/Liso et al. - 2024 - Loopy-SLAM Dense Neural SLAM with Loop Closures.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/9TUKQ2K7/2402.html:text/html}
}

@misc{tang_mips-fusion_2023,
  title      = {{MIPS}-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online Neural {RGB}-D Reconstruction},
  url        = {http://arxiv.org/abs/2308.08741},
  doi        = {10.48550/arXiv.2308.08741},
  shorttitle = {{MIPS}-Fusion},
  abstract   = {We introduce {MIPS}-Fusion, a robust and scalable online {RGB}-D reconstruction method based on a novel neural implicit representation -- multi-implicit-submap. Different from existing neural {RGB}-D reconstruction methods lacking either flexibility with a single neural map or scalability due to extra storage of feature grids, we propose a pure neural representation tackling both difficulties with a divide-and-conquer design. In our method, neural submaps are incrementally allocated alongside the scanning trajectory and efficiently learned with local neural bundle adjustments. The submaps can be refined individually in a back-end optimization and optimized jointly to realize submap-level loop closure. Meanwhile, we propose a hybrid tracking approach combining randomized and gradient-based pose optimizations. For the first time, randomized optimization is made possible in neural tracking with several key designs to the learning process, enabling efficient and robust tracking even under fast camera motions. The extensive evaluation demonstrates that our method attains higher reconstruction quality than the state of the arts for large-scale scenes and under fast camera motions.},
  number     = {{arXiv}:2308.08741},
  publisher  = {{arXiv}},
  author     = {Tang, Yijie and Zhang, Jiazhao and Yu, Zhinan and Wang, He and Xu, Kai},
  urldate    = {2025-01-24},
  date       = {2023-08-24},
  eprinttype = {arxiv},
  eprint     = {2308.08741 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/AUUAG4AB/Tang et al. - 2023 - MIPS-Fusion Multi-Implicit-Submaps for Scalable and Robust Online Neural RGB-D Reconstruction.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/NLYF6F3P/2308.html:text/html}
}

@article{mur-artal_orb-slam_2015,
  title        = {{ORB}-{SLAM}: A Versatile and Accurate Monocular {SLAM} System},
  volume       = {31},
  issn         = {1941-0468},
  url          = {https://ieeexplore.ieee.org/abstract/document/7219438?casa_token=LBweiJCy0HEAAAAA:3aOxYeYqbadYqT3WCYyQQXCVsplEmf6zRwKffZ4DOLM7zx9TxGiMPw4T4rotNFg9B0Rs8so},
  doi          = {10.1109/TRO.2015.2463671},
  shorttitle   = {{ORB}-{SLAM}},
  abstract     = {This paper presents {ORB}-{SLAM}, a feature-based monocular simultaneous localization and mapping ({SLAM}) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all {SLAM} tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. {ORB}-{SLAM} achieves unprecedented performance with respect to other state-of-the-art monocular {SLAM} approaches. For the benefit of the community, we make the source code public.},
  pages        = {1147--1163},
  number       = {5},
  journaltitle = {{IEEE} Transactions on Robotics},
  author       = {Mur-Artal, Raúl and Montiel, J. M. M. and Tardós, Juan D.},
  urldate      = {2025-01-26},
  date         = {2015-10},
  note         = {{IEEE} Transactions on Robotics},
  keywords     = {Optimization, Real-time systems, Feature extraction, Computational modeling, Visualization, Cameras, localization, Simultaneous localization and mapping, simultaneous localization and mapping ({SLAM}), Lifelong mapping, monocular vision, recognition},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/ITE32487/Mur-Artal et al. - 2015 - ORB-SLAM A Versatile and Accurate Monocular SLAM System.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/guillemcv/AppData/Zotero/storage/VJR72XPU/7219438.html:text/html}
}

@inproceedings{newcombe_kinectfusion_2011,
  title      = {{KinectFusion}: Real-time dense surface mapping and tracking},
  url        = {https://ieeexplore.ieee.org/document/6162880/?arnumber=6162880},
  doi        = {10.1109/ISMAR.2011.6092378},
  shorttitle = {{KinectFusion}},
  abstract   = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point ({ICP}) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and {GPU} hardware, promises an exciting step forward in augmented reality ({AR}), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  eventtitle = {2011 10th {IEEE} International Symposium on Mixed and Augmented Reality},
  pages      = {127--136},
  booktitle  = {2011 10th {IEEE} International Symposium on Mixed and Augmented Reality},
  author     = {Newcombe, Richard A. and Izadi, Shahram and Hilliges, Otmar and Molyneaux, David and Kim, David and Davison, Andrew J. and Kohi, Pushmeet and Shotton, Jamie and Hodges, Steve and Fitzgibbon, Andrew},
  urldate    = {2025-01-26},
  date       = {2011-10},
  keywords   = {Cameras, Real time systems, Surface reconstruction, Simultaneous localization and mapping, Image reconstruction, {AR}, Dense Reconstruction, Depth Cameras, {GPU}, Iterative closest point algorithm, Real-Time, {SLAM}, Three dimensional displays, Tracking, Volumetric Representation},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/6L9Y8MIV/Newcombe et al. - 2011 - KinectFusion Real-time dense surface mapping and tracking.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/guillemcv/AppData/Zotero/storage/SH4TT6V5/6162880.html:text/html}
}

@inproceedings{tateno_cnn-slam_2017,
  location   = {Honolulu, {HI}},
  title      = {{CNN}-{SLAM}: Real-Time Dense Monocular {SLAM} with Learned Depth Prediction},
  isbn       = {978-1-5386-0457-1},
  url        = {http://ieeexplore.ieee.org/document/8100178/},
  doi        = {10.1109/CVPR.2017.695},
  shorttitle = {{CNN}-{SLAM}},
  abstract   = {Given the recent advances in depth prediction from Convolutional Neural Networks ({CNNs}), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where {CNN}-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular {SLAM}. Our fusion scheme privileges depth prediction in image locations where monocular {SLAM} approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular {SLAM}. Finally, we propose a framework to efﬁciently fuse semantic labels, obtained from a single frame, with dense {SLAM}, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.},
  eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages      = {6565--6574},
  booktitle  = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher  = {{IEEE}},
  author     = {Tateno, Keisuke and Tombari, Federico and Laina, Iro and Navab, Nassir},
  urldate    = {2025-01-26},
  date       = {2017-07},
  langid     = {english},
  file       = {PDF:/Users/guillemcv/AppData/Zotero/storage/DWPPAH2Y/Tateno et al. - 2017 - CNN-SLAM Real-Time Dense Monocular SLAM with Learned Depth Prediction.pdf:application/pdf}
}

@inproceedings{teed_droid-slam_2021,
  title      = {{DROID}-{SLAM}: Deep Visual {SLAM} for Monocular, Stereo, and {RGB}-D Cameras},
  volume     = {34},
  url        = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/89fcd07f20b6785b92134bd6c1d0fa42-Abstract.html},
  shorttitle = {{DROID}-{SLAM}},
  abstract   = {We introduce {DROID}-{SLAM}, a new deep learning based {SLAM} system. {DROID}-{SLAM} consists of recurrent iterative updates of camera pose and pixelwise depth through a Dense Bundle Adjustment layer. {DROID}-{SLAM} is accurate, achieving large improvements over prior work, and robust, suffering from substantially fewer catastrophic failures. Despite training on monocular video, it can leverage stereo or {RGB}-D video to achieve improved performance at test time. The {URL} to our open source code is https://github.com/princeton-vl/{DROID}-{SLAM}.},
  pages      = {16558--16569},
  booktitle  = {Advances in Neural Information Processing Systems},
  publisher  = {Curran Associates, Inc.},
  author     = {Teed, Zachary and Deng, Jia},
  urldate    = {2025-01-26},
  date       = {2021},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/4PSBXL3K/Teed and Deng - 2021 - DROID-SLAM Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras.pdf:application/pdf}
}

@inproceedings{park_deepsdf_2019,
  title      = {{DeepSDF}: Learning Continuous Signed Distance Functions for Shape Representation},
  url        = {https://openaccess.thecvf.com/content_CVPR_2019/html/Park_DeepSDF_Learning_Continuous_Signed_Distance_Functions_for_Shape_Representation_CVPR_2019_paper.html},
  shorttitle = {{DeepSDF}},
  eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
  pages      = {165--174},
  author     = {Park, Jeong Joon and Florence, Peter and Straub, Julian and Newcombe, Richard and Lovegrove, Steven},
  urldate    = {2025-01-26},
  date       = {2019},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/E856EZFZ/Park et al. - 2019 - DeepSDF Learning Continuous Signed Distance Functions for Shape Representation.pdf:application/pdf}
}

@inproceedings{mescheder_occupancy_2019,
  title      = {Occupancy Networks: Learning 3D Reconstruction in Function Space},
  url        = {https://openaccess.thecvf.com/content_CVPR_2019/html/Mescheder_Occupancy_Networks_Learning_3D_Reconstruction_in_Function_Space_CVPR_2019_paper.html},
  shorttitle = {Occupancy Networks},
  eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
  pages      = {4460--4470},
  author     = {Mescheder, Lars and Oechsle, Michael and Niemeyer, Michael and Nowozin, Sebastian and Geiger, Andreas},
  urldate    = {2025-01-26},
  date       = {2019},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/L6HRDGA7/Mescheder et al. - 2019 - Occupancy Networks Learning 3D Reconstruction in Function Space.pdf:application/pdf}
}

@inproceedings{kirillov_segment_2023,
  title      = {Segment Anything},
  url        = {https://openaccess.thecvf.com/content/ICCV2023/html/Kirillov_Segment_Anything_ICCV_2023_paper.html},
  eventtitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
  pages      = {4015--4026},
  author     = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollar, Piotr and Girshick, Ross},
  urldate    = {2025-01-26},
  date       = {2023},
  langid     = {english},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/CNWC9TUD/Kirillov et al. - 2023 - Segment Anything.pdf:application/pdf}
}

@misc{sandstrom_splat-slam_2024,
  title      = {Splat-{SLAM}: Globally Optimized {RGB}-only {SLAM} with 3D Gaussians},
  url        = {http://arxiv.org/abs/2405.16544},
  doi        = {10.48550/arXiv.2405.16544},
  shorttitle = {Splat-{SLAM}},
  abstract   = {3D Gaussian Splatting has emerged as a powerful representation of geometry and appearance for {RGB}-only dense Simultaneous Localization and Mapping ({SLAM}), as it provides a compact dense map representation while enabling efficient and high-quality map rendering. However, existing methods show significantly worse reconstruction quality than competing methods using other 3D representations, e.g. neural points clouds, since they either do not employ global map and pose optimization or make use of monocular depth. In response, we propose the first {RGB}-only {SLAM} system with a dense 3D Gaussian map representation that utilizes all benefits of globally optimized tracking by adapting dynamically to keyframe pose and depth updates by actively deforming the 3D Gaussian map. Moreover, we find that refining the depth updates in inaccurate areas with a monocular depth estimator further improves the accuracy of the 3D reconstruction. Our experiments on the Replica, {TUM}-{RGBD}, and {ScanNet} datasets indicate the effectiveness of globally optimized 3D Gaussians, as the approach achieves superior or on par performance with existing {RGB}-only {SLAM} methods methods in tracking, mapping and rendering accuracy while yielding small map sizes and fast runtimes. The source code is available at https://github.com/eriksandstroem/Splat-{SLAM}.},
  number     = {{arXiv}:2405.16544},
  publisher  = {{arXiv}},
  author     = {Sandström, Erik and Tateno, Keisuke and Oechsle, Michael and Niemeyer, Michael and Gool, Luc Van and Oswald, Martin R. and Tombari, Federico},
  urldate    = {2025-01-29},
  date       = {2024-05-26},
  eprinttype = {arxiv},
  eprint     = {2405.16544 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/97XACP5W/Sandström et al. - 2024 - Splat-SLAM Globally Optimized RGB-only SLAM with 3D Gaussians.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/JAWMM6FP/2405.html:text/html}
}

@misc{zhu_loopsplat_2024,
  title      = {{LoopSplat}: Loop Closure by Registering 3D Gaussian Splats},
  url        = {http://arxiv.org/abs/2408.10154},
  doi        = {10.48550/arXiv.2408.10154},
  shorttitle = {{LoopSplat}},
  abstract   = {Simultaneous Localization and Mapping ({SLAM}) based on 3D Gaussian Splats (3DGS) has recently shown promise towards more accurate, dense 3D scene maps. However, existing 3DGS-based methods fail to address the global consistency of the scene via loop closure and/or global bundle adjustment. To this end, we propose {LoopSplat}, which takes {RGB}-D images as input and performs dense mapping with 3DGS submaps and frame-to-model tracking. {LoopSplat} triggers loop closure online and computes relative loop edge constraints between submaps directly via 3DGS registration, leading to improvements in efficiency and accuracy over traditional global-to-local point cloud registration. It uses a robust pose graph optimization formulation and rigidly aligns the submaps to achieve global consistency. Evaluation on the synthetic Replica and real-world {TUM}-{RGBD}, {ScanNet}, and {ScanNet}++ datasets demonstrates competitive or superior tracking, mapping, and rendering compared to existing methods for dense {RGB}-D {SLAM}. Code is available at loopsplat.github.io.},
  number     = {{arXiv}:2408.10154},
  publisher  = {{arXiv}},
  author     = {Zhu, Liyuan and Li, Yue and Sandström, Erik and Huang, Shengyu and Schindler, Konrad and Armeni, Iro},
  urldate    = {2025-01-29},
  date       = {2024-08-20},
  eprinttype = {arxiv},
  eprint     = {2408.10154 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/V6HQG52X/Zhu et al. - 2024 - LoopSplat Loop Closure by Registering 3D Gaussian Splats.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/8UQ7VYDP/2408.html:text/html}
}

@misc{matsuki_gaussian_2024,
  title      = {Gaussian Splatting {SLAM}},
  url        = {http://arxiv.org/abs/2312.06741},
  doi        = {10.48550/arXiv.2312.06741},
  abstract   = {We present the first application of 3D Gaussian Splatting in monocular {SLAM}, the most fundamental but the hardest setup for Visual {SLAM}. Our method, which runs live at 3fps, utilises Gaussians as the only 3D representation, unifying the required representation for accurate, efficient tracking, mapping, and high-quality rendering. Designed for challenging monocular settings, our approach is seamlessly extendable to {RGB}-D {SLAM} when an external depth sensor is available. Several innovations are required to continuously reconstruct 3D scenes with high fidelity from a live camera. First, to move beyond the original 3DGS algorithm, which requires accurate poses from an offline Structure from Motion ({SfM}) system, we formulate camera tracking for 3DGS using direct optimisation against the 3D Gaussians, and show that this enables fast and robust tracking with a wide basin of convergence. Second, by utilising the explicit nature of the Gaussians, we introduce geometric verification and regularisation to handle the ambiguities occurring in incremental 3D dense reconstruction. Finally, we introduce a full {SLAM} system which not only achieves state-of-the-art results in novel view synthesis and trajectory estimation but also reconstruction of tiny and even transparent objects.},
  number     = {{arXiv}:2312.06741},
  publisher  = {{arXiv}},
  author     = {Matsuki, Hidenobu and Murai, Riku and Kelly, Paul H. J. and Davison, Andrew J.},
  urldate    = {2025-02-05},
  date       = {2024-04-14},
  eprinttype = {arxiv},
  eprint     = {2312.06741 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/ZQLLSE89/Matsuki et al. - 2024 - Gaussian Splatting SLAM.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/PUAGULDC/2312.html:text/html}
}

@inproceedings{matsuki_gaussian_2024-1,
  title      = {Gaussian Splatting {SLAM}},
  url        = {https://openaccess.thecvf.com/content/CVPR2024/html/Matsuki_Gaussian_Splatting_SLAM_CVPR_2024_paper.html},
  eventtitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
  pages      = {18039--18048},
  author     = {Matsuki, Hidenobu and Murai, Riku and Kelly, Paul H. J. and Davison, Andrew J.},
  urldate    = {2025-02-05},
  date       = {2024},
  langid     = {english},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/X539L44Q/Matsuki et al. - 2024 - Gaussian Splatting SLAM.pdf:application/pdf}
}

@inproceedings{sucar_imap_2021,
  title      = {{iMAP}: Implicit Mapping and Positioning in Real-Time},
  url        = {https://openaccess.thecvf.com/content/ICCV2021/html/Sucar_iMAP_Implicit_Mapping_and_Positioning_in_Real-Time_ICCV_2021_paper.html},
  shorttitle = {{iMAP}},
  eventtitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
  pages      = {6229--6238},
  author     = {Sucar, Edgar and Liu, Shikun and Ortiz, Joseph and Davison, Andrew J.},
  urldate    = {2025-02-05},
  date       = {2021},
  langid     = {english},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/WCFBBS5H/Sucar et al. - 2021 - iMAP Implicit Mapping and Positioning in Real-Time.pdf:application/pdf}
}

@inproceedings{sandstrom_point-slam_2023,
  title      = {Point-{SLAM}: Dense Neural Point Cloud-based {SLAM}},
  url        = {https://openaccess.thecvf.com/content/ICCV2023/html/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.html},
  shorttitle = {Point-{SLAM}},
  eventtitle = {Proceedings of the {IEEE}/{CVF} International Conference on Computer Vision},
  pages      = {18433--18444},
  author     = {Sandström, Erik and Li, Yue and Van Gool, Luc and Oswald, Martin R.},
  urldate    = {2025-02-05},
  date       = {2023},
  langid     = {english},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/WRQ45D4I/Sandström et al. - 2023 - Point-SLAM Dense Neural Point Cloud-based SLAM.pdf:application/pdf}
}

@misc{zhu_nice-slam_2022,
  title      = {{NICE}-{SLAM}: Neural Implicit Scalable Encoding for {SLAM}},
  url        = {http://arxiv.org/abs/2112.12130},
  doi        = {10.48550/arXiv.2112.12130},
  shorttitle = {{NICE}-{SLAM}},
  abstract   = {Neural implicit representations have recently shown encouraging results in various domains, including promising progress in simultaneous localization and mapping ({SLAM}). Nevertheless, existing methods produce over-smoothed scene reconstructions and have difficulty scaling up to large scenes. These limitations are mainly due to their simple fully-connected network architecture that does not incorporate local information in the observations. In this paper, we present {NICE}-{SLAM}, a dense {SLAM} system that incorporates multi-level local information by introducing a hierarchical scene representation. Optimizing this representation with pre-trained geometric priors enables detailed reconstruction on large indoor scenes. Compared to recent neural implicit {SLAM} systems, our approach is more scalable, efficient, and robust. Experiments on five challenging datasets demonstrate competitive results of {NICE}-{SLAM} in both mapping and tracking quality. Project page: https://pengsongyou.github.io/nice-slam},
  number     = {{arXiv}:2112.12130},
  publisher  = {{arXiv}},
  author     = {Zhu, Zihan and Peng, Songyou and Larsson, Viktor and Xu, Weiwei and Bao, Hujun and Cui, Zhaopeng and Oswald, Martin R. and Pollefeys, Marc},
  urldate    = {2025-02-05},
  date       = {2022-04-21},
  eprinttype = {arxiv},
  eprint     = {2112.12130 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/G94TCCTS/Zhu et al. - 2022 - NICE-SLAM Neural Implicit Scalable Encoding for SLAM.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/6U3LSA2Q/2112.html:text/html}
}

@misc{zhou_open3d_2018,
  title      = {Open3D: A Modern Library for 3D Data Processing},
  url        = {http://arxiv.org/abs/1801.09847},
  doi        = {10.48550/arXiv.1801.09847},
  shorttitle = {Open3D},
  abstract   = {Open3D is an open-source library that supports rapid development of software that deals with 3D data. The Open3D frontend exposes a set of carefully selected data structures and algorithms in both C++ and Python. The backend is highly optimized and is set up for parallelization. Open3D was developed from a clean slate with a small and carefully considered set of dependencies. It can be set up on different platforms and compiled from source with minimal effort. The code is clean, consistently styled, and maintained via a clear code review mechanism. Open3D has been used in a number of published research projects and is actively deployed in the cloud. We welcome contributions from the open-source community.},
  number     = {{arXiv}:1801.09847},
  publisher  = {{arXiv}},
  author     = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
  urldate    = {2025-03-17},
  date       = {2018-01-30},
  eprinttype = {arxiv},
  eprint     = {1801.09847 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics, Computer Science - Graphics},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/ZABFD6AS/Zhou et al. - 2018 - Open3D A Modern Library for 3D Data Processing.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/M77CL9PC/1801.html:text/html}
}

@article{duchon_path_2014,
  title        = {Path Planning with Modified a Star Algorithm for a Mobile Robot},
  volume       = {96},
  issn         = {1877-7058},
  url          = {https://www.sciencedirect.com/science/article/pii/S187770581403149X},
  doi          = {10.1016/j.proeng.2014.12.098},
  series       = {Modelling of Mechanical and Mechatronic Systems},
  abstract     = {This article deals with path planning of a mobile robot based on a grid map. Essential assumption for path planning is a mobile robot with functional and reliable reactive navigation and {SLAM}. Therefore, such issues are not addressed in this article. The main body of the article introduces several modifications (Basic Theta*, Phi*) and improvements ({RSR}, {JPS}) of A star algorithm. These modifications are focused primarily on computational time and the path optimality. Individual modifications were evaluated in several scenarios, which varied in the complexity of environment. On the basis of these evaluations, it is possible to choose path planning method suitable for individual scenario.},
  pages        = {59--69},
  journaltitle = {Procedia Engineering},
  shortjournal = {Procedia Engineering},
  author       = {Duchoň, František and Babinec, Andrej and Kajan, Martin and Beňo, Peter and Florek, Martin and Fico, Tomáš and Jurišica, Ladislav},
  urldate      = {2025-03-17},
  date         = {2014-01-01},
  keywords     = {A* algorithm, Basic Theta*, Jump Point Search, Path planning, Phi*},
  file         = {ScienceDirect Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/32AF4LIU/Duchoň et al. - 2014 - Path Planning with Modified a Star Algorithm for a Mobile Robot.pdf:application/pdf;ScienceDirect Snapshot:/Users/guillemcv/AppData/Zotero/storage/2XDWP8BL/S187770581403149X.html:text/html}
}

@online{noauthor_chengine_nodate,
  title   = {chengine},
  url     = {https://github.com/chengine},
  urldate = {2025-03-18},
  langid  = {english},
  file    = {Snapshot:/Users/guillemcv/AppData/Zotero/storage/SIUVKN73/chengine.html:text/html}
}

@article{zhang_hilti-oxford_2023,
  title        = {Hilti-Oxford Dataset: A Millimetre-Accurate Benchmark for Simultaneous Localization and Mapping},
  volume       = {8},
  issn         = {2377-3766, 2377-3774},
  url          = {http://arxiv.org/abs/2208.09825},
  doi          = {10.1109/LRA.2022.3226077},
  shorttitle   = {Hilti-Oxford Dataset},
  abstract     = {Simultaneous Localization and Mapping ({SLAM}) is being deployed in real-world applications, however many state-of-the-art solutions still struggle in many common scenarios. A key necessity in progressing {SLAM} research is the availability of high-quality datasets and fair and transparent benchmarking. To this end, we have created the Hilti-Oxford Dataset, to push state-of-the-art {SLAM} systems to their limits. The dataset has a variety of challenges ranging from sparse and regular construction sites to a 17th century neoclassical building with fine details and curved surfaces. To encourage multi-modal {SLAM} approaches, we designed a data collection platform featuring a lidar, five cameras, and an {IMU} (Inertial Measurement Unit). With the goal of benchmarking {SLAM} algorithms for tasks where accuracy and robustness are paramount, we implemented a novel ground truth collection method that enables our dataset to accurately measure {SLAM} pose errors with millimeter accuracy. To further ensure accuracy, the extrinsics of our platform were verified with a micrometer-accurate scanner, and temporal calibration was managed online using hardware time synchronization. The multi-modality and diversity of our dataset attracted a large field of academic and industrial researchers to enter the second edition of the Hilti {SLAM} challenge, which concluded in June 2022. The results of the challenge show that while the top three teams could achieve an accuracy of 2cm or better for some sequences, the performance dropped off in more difficult sequences.},
  pages        = {408--415},
  number       = {1},
  journaltitle = {{IEEE} Robotics and Automation Letters},
  shortjournal = {{IEEE} Robot. Autom. Lett.},
  author       = {Zhang, Lintong and Helmberger, Michael and Fu, Lanke Frank Tarimo and Wisth, David and Camurri, Marco and Scaramuzza, Davide and Fallon, Maurice},
  urldate      = {2025-03-27},
  date         = {2023-01},
  eprinttype   = {arxiv},
  eprint       = {2208.09825 [cs]},
  keywords     = {Computer Science - Robotics, Electrical Engineering and Systems Science - Image and Video Processing},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/WK3MHZ5Y/Zhang et al. - 2023 - Hilti-Oxford Dataset A Millimetre-Accurate Benchmark for Simultaneous Localization and Mapping.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/JZWS9LC2/2208.html:text/html}
}


@misc{euroconsult_prospects_2023,
  title    = {Prospects for Space Exploration, 4th edition},
  url      = {https://digital-platform.euroconsult-ec.com/product/prospects-for-space-exploration/},
  abstract = {Prospects for Space Exploration is an economic and strategic assessment of the space exploration sector that includes an analysis and benchmark.},
  author   = {{Euroconsult}},
  urldate  = {2024-01-27},
  date     = {2023-09},
  langid   = {american},
  file     = {Snapshot:/Users/guillemcv/AppData/Zotero/storage/QAFSTXBK/prospects-for-space-exploration.html:text/html;SPX_23_Extract.pdf:/Users/guillemcv/AppData/Zotero/storage/7LGGHSHH/SPX_23_Extract.pdf:application/pdf}
}


@inproceedings{dai_neural_2023,
  title      = {Neural Radiance Maps for Extraterrestrial Navigation and Path Planning},
  url        = {http://www.ion.org/publications/abstract.cfm?jp=p&articleID=19202},
  doi        = {10.33012/2023.19202},
  abstract   = {Autonomous vehicles such as the Mars rovers currently lead the vanguard of surface exploration on extraterrestrial planets and moons. In order to accelerate the pace of exploration and science objectives, it is critical to plan safe and efficient paths for these vehicles. However, current rover autonomy is limited by a lack of global maps which can be easily constructed and stored for onboard re-planning. Recently, Neural Radiance Fields ({NeRFs}) have been introduced as a detailed 3D scene representation which can be trained from sparse 2D images and efficiently stored. We propose to use {NeRFs} to construct maps for online use in autonomous navigation, and present a planning framework which leverages the {NeRF} map to integrate local and global information. Our approach interpolates local cost observations across global regions using kernel ridge regression over terrain features extracted from the {NeRF} map, allowing the rover to re-route itself around untraversable areas discovered during online operation. We validate our approach in high-fidelity simulation and demonstrate lower cost and higher percentage success rate path planning compared to various baselines.},
  eventtitle = {Proceedings of the 36th International Technical Meeting of the Satellite Division of The Institute of Navigation ({ION} {GNSS}+ 2023)},
  pages      = {1606--1620},
  author     = {Dai, Adam and Gupta, Shubh and Gao, Grace},
  urldate    = {2025-10-04},
  date       = {2023-09-15},
  langid     = {english},
  note       = {{ISSN}: 2331-5954}
}


@inproceedings{van_kints_neural_2025,
  title      = {Neural Radiance Methods for Lunar Terrain Modeling},
  url        = {https://ieeexplore.ieee.org/abstract/document/11068396},
  doi        = {10.1109/AERO63441.2025.11068396},
  abstract   = {Recent advancements in scene representation, such as Neural Radiance Fields ({NeRFs}), provide continuous representations of objects and scenes using coordinate-based neural networks. These methods are effective for Digital Elevation Model ({DEM}) reconstruction but struggle with accurately modeling largely shadowed regions due to their lack of depth-supervision and shadow-awareness. To address this, we aim to develop Lunar Neural Radiance Methods ({LunarNRM}), a novel neural surface reconstruction method that incorporates shadow-aware and depth-aware techniques into a {NeRF} pipeline. {LunarNRM} will generate shadow-controlled {DEMs} of the lunar surface, allowing for the relighting of topographic depressions such as craters. By incorporating multi-sensor data from the Lunar Reconnaissance Orbiter ({LRO}) satellite, specifically optical data from the Narrow Angle Camera ({NAC}) and altimeter data from the Lunar Orbiter Laser Altimeter ({LOLA}), we aim to demonstrate that {LunarNRM} can reconstruct largely shadowed regions in the lunar South Pole, which are critical targets for the {NASA} Artemis campaign.},
  eventtitle = {2025 {IEEE} Aerospace Conference},
  pages      = {1--17},
  booktitle  = {2025 {IEEE} Aerospace Conference},
  author     = {Van Kints, Ellemieke and Hammond, Aiden and Adams, Caleb and Lopez-Francos, Ignacio G.},
  urldate    = {2025-10-04},
  date       = {2025-03},
  note       = {{ISSN}: 2996-2358},
  keywords   = {Cameras, Image reconstruction, Moon, Neural radiance field, Noise, Orbits, Predictive models, Surface reconstruction, Surface topography, Training},
  file       = {Snapshot:/Users/guillemcv/AppData/Zotero/storage/XXYW4V8A/11068396.html:text/html}
}


@article{huang_monocular_2025,
  title        = {Monocular Visual {SLAM} With Adjusting Neural Radiance Fields for 3-D Reconstruction in Planetary Environments},
  volume       = {63},
  issn         = {1558-0644},
  url          = {https://ieeexplore.ieee.org/abstract/document/11023632},
  doi          = {10.1109/TGRS.2025.3574514},
  abstract     = {In planetary environments, conducting autonomous exploration tasks requires rovers to autonomously navigate the scene and achieve a detailed understanding of the terrain. Vision-based simultaneous localization and mapping ({SLAM}), which utilizes compact and low-power visual sensors for autonomous exploration, offers significant advantages in hardware deployment. While several methods have been proposed to apply visual navigation in planetary scenarios, they often rely on aerial imagery from orbiters and high-resolution digital elevation models ({DEMs}) for assistance. In addition, accurate camera poses are typically required for dense matching during scene reconstruction, and the inability to perform loop closure significantly limits the performance of visual {SLAM}. Here, we propose a monocular visual {SLAM} approach combined with an adjusted neural radiance field ({NeRF}) for autonomous navigation and 3-D reconstruction in planetary environments. Our approach solely relies on visual images as input and leverages the powerful learning capabilities of {NeRFs} to adapt to unseen scenes while simultaneously regressing both camera poses and scene representations. The estimated depth maps and poses can be further used for 3-D reconstruction, assisting planetary exploration missions. The proposed method was tested on the Devon and {MADMAX} datasets that simulate planetary environments and achieved remarkable results. Even under the fixed rover navigation perspective, our pose estimation accuracy outperforms classical visual {SLAM} and other deep learning-based {SLAM} methods. In addition, our novel view synthesis results exhibit quality comparable to those in terrestrial scenes. Comparisons with multiview stereo ({MVS}) techniques in terms of 3-D reconstruction demonstrate that our approach recovers finer surface details. We also applied our method to the Perseverance rover dataset and achieved satisfactory positioning and reconstruction results in a real Martian environment, proving the practical feasibility of our method.},
  pages        = {1--19},
  journaltitle = {{IEEE} Transactions on Geoscience and Remote Sensing},
  author       = {Huang, Rong and Liu, Chen and Xie, Huan and Yu, Jiyang and Tao, Tao and Xu, Yusheng and Ye, Zhen and Tong, Xiaohua},
  urldate      = {2025-10-04},
  date         = {2025},
  keywords     = {3-D reconstruction, Accuracy, Cameras, Feature extraction, Image reconstruction, Navigation, Neural radiance field, neural radiance fields ({NeRFs}), planetary environments, Reconstruction algorithms, Simultaneous localization and mapping, Three-dimensional displays, visual simultaneous localization and mapping ({SLAM}), Visualization}
}



@article{adams_summary_2023,
  title        = {A Summary of Neural Radiance Fields for Shadow Removal and Relighting of Satellite Imagery},
  url          = {https://digitalcommons.usu.edu/smallsat/2023/all2023/9},
  journaltitle = {Small Satellite Conference},
  author       = {Adams, Caleb and Lopez-Francos, Ignacio and Kints, Ellemieke Van and Hammond, Aiden},
  date         = {2023-08-05},
  file         = {text/html Attachment:/Users/guillemcv/AppData/Zotero/storage/J2FY38CV/9.html:text/html}
}



@inproceedings{hansen_analyzing_2024,
  title      = {Analyzing the Effectiveness of Neural Radiance Fields for Geometric Modeling of Lunar Terrain},
  url        = {https://ieeexplore.ieee.org/abstract/document/10521163},
  doi        = {10.1109/AERO58975.2024.10521163},
  abstract   = {The geometric accuracy of digital elevation models built from neural radiance fields ({NeRFs}) is assessed using stereo pairs collected during a simulated rover traverse under lunar polar lighting conditions by comparison to multi-view stereo reconstruction. While {NeRF}-based methods are more sensitive to the viewpoints in the training data and produce more artifacts on the edges of the scene, they are capable of producing denser models in occluded regions with limited additional error when the light source is not visible in the cameras. With a visible light source, the {NeRF} models are incapable of correctly learning scene geometry, though rendered images still appear to be decent. This trend is mitigated somewhat by using depth supervision, though this method elsewhere produces higher amounts of error. Since the volumetric rendering used by {NeRF} relies on probabilistic reasoning along the ray used to observe the scene, the standard deviation and gradient of the cumulative distribution function can be used as indicators of how sharply a {NeRF} model resolves a surface and are correlated with height error.},
  eventtitle = {2024 {IEEE} Aerospace Conference},
  pages      = {1--12},
  booktitle  = {2024 {IEEE} Aerospace Conference},
  author     = {Hansen, Margaret and Adams, Caleb and Fong, Terrence and Wettergreen, David},
  urldate    = {2025-10-04},
  date       = {2024-03},
  note       = {{ISSN}: 1095-323X},
  keywords   = {Cameras, Geometry, Lighting, Moon, Noise, Rendering (computer graphics), Runtime},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/H8VCXLAV/Hansen et al. - 2024 - Analyzing the Effectiveness of Neural Radiance Fields for Geometric Modeling of Lunar Terrain.pdf:application/pdf}
}


@inproceedings{zhang_neural_2024,
  location   = {Yokohama, Japan},
  title      = {Neural Radiance Fields for Unbounded Lunar Surface Scene},
  rights     = {https://doi.org/10.15223/policy-029},
  isbn       = {979-8-3503-8457-4},
  url        = {https://ieeexplore.ieee.org/document/10611137/},
  doi        = {10.1109/ICRA57147.2024.10611137},
  abstract   = {Accurate understanding of lunar surface topography is vital for effective decision-making and remote control of lunar rovers during exploration missions. Conventional sensing methods often struggle to capture the intricate details of the lunar landscape. In response, we propose an innovative approach that leverages {NeRF} to synthesize new viewpoints within the expansive lunar environment. By blending 3D hash grids and 2D plane grids representations, our approach provides a comprehensive scene representation. We employ the technique of spiral sampling and feature rendering to enhance rendering quality while simultaneously reducing training time. Additionally, we leverage sparse point cloud to aid the model in better learning the geometric structure of the lunar environment. Through experimentation, we have demonstrated that our method is capable of synthesizing realistic images of lunar environments.},
  eventtitle = {2024 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  pages      = {16858--16864},
  booktitle  = {2024 {IEEE} International Conference on Robotics and Automation ({ICRA})},
  publisher  = {{IEEE}},
  author     = {Zhang, Xu and Cui, Linyan and Yin, Jihao},
  urldate    = {2024-10-07},
  date       = {2024-05-13},
  langid     = {english},
  file       = {PDF:/Users/guillemcv/AppData/Zotero/storage/FNK2XT29/Zhang et al. - 2024 - Neural Radiance Fields for Unbounded Lunar Surface Scene.pdf:application/pdf}
}



@inproceedings{lin_feature_2017,
  title      = {Feature Pyramid Networks for Object Detection},
  url        = {https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Feature_Pyramid_Networks_CVPR_2017_paper.html},
  eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
  pages      = {2117--2125},
  author     = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
  urldate    = {2025-10-03},
  date       = {2017},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/GVXB9XW9/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf}
}

@article{campos_orb-slam3_2021,
  title        = {{ORB}-{SLAM}3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map {SLAM}},
  volume       = {37},
  issn         = {1552-3098, 1941-0468},
  url          = {http://arxiv.org/abs/2007.11898},
  doi          = {10.1109/TRO.2021.3075644},
  shorttitle   = {{ORB}-{SLAM}3},
  abstract     = {This paper presents {ORB}-{SLAM}3, the first system able to perform visual, visual-inertial and multi-map {SLAM} with monocular, stereo and {RGB}-D cameras, using pin-hole and fisheye lens models. The first main novelty is a feature-based tightly-integrated visual-inertial {SLAM} system that fully relies on Maximum-a-Posteriori ({MAP}) estimation, even during the {IMU} initialization phase. The result is a system that operates robustly in real-time, in small and large, indoor and outdoor environments, and is 2 to 5 times more accurate than previous approaches. The second main novelty is a multiple map system that relies on a new place recognition method with improved recall. Thanks to it, {ORB}-{SLAM}3 is able to survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting mapped areas. Compared with visual odometry systems that only use information from the last few seconds, {ORB}-{SLAM}3 is the first system able to reuse in all the algorithm stages all previous information. This allows to include in bundle adjustment co-visible keyframes, that provide high parallax observations boosting accuracy, even if they are widely separated in time or if they come from a previous mapping session. Our experiments show that, in all sensor configurations, {ORB}-{SLAM}3 is as robust as the best systems available in the literature, and significantly more accurate. Notably, our stereo-inertial {SLAM} achieves an average accuracy of 3.6 cm on the {EuRoC} drone and 9 mm under quick hand-held motions in the room of {TUM}-{VI} dataset, a setting representative of {AR}/{VR} scenarios. For the benefit of the community we make public the source code.},
  pages        = {1874--1890},
  number       = {6},
  journaltitle = {{IEEE} Transactions on Robotics},
  shortjournal = {{IEEE} Trans. Robot.},
  author       = {Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and Montiel, José M. M. and Tardós, Juan D.},
  urldate      = {2025-03-28},
  date         = {2021-12},
  eprinttype   = {arxiv},
  eprint       = {2007.11898 [cs]},
  keywords     = {Computer Science - Robotics},
  file         = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/SCCVPFTV/Campos et al. - 2021 - ORB-SLAM3 An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/DM9WZTSF/2007.html:text/html}
}

@inproceedings{schonberger_structure--motion_2016,
  title      = {Structure-From-Motion Revisited},
  url        = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Schonberger_Structure-From-Motion_Revisited_CVPR_2016_paper.html},
  eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
  pages      = {4104--4113},
  author     = {Schonberger, Johannes L. and Frahm, Jan-Michael},
  urldate    = {2025-04-25},
  date       = {2016},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/M63LDZC7/Schonberger and Frahm - 2016 - Structure-From-Motion Revisited.pdf:application/pdf}
}

@misc{oshea_introduction_2015,
  title      = {An Introduction to Convolutional Neural Networks},
  url        = {http://arxiv.org/abs/1511.08458},
  doi        = {10.48550/arXiv.1511.08458},
  abstract   = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network ({ANN}). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of {ANN} architecture is that of the Convolutional Neural Network ({CNN}). {CNNs} are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with {ANNs}. This document provides a brief introduction to {CNNs}, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of {ANNs} and machine learning.},
  number     = {{arXiv}:1511.08458},
  publisher  = {{arXiv}},
  author     = {O'Shea, Keiron and Nash, Ryan},
  urldate    = {2025-04-25},
  date       = {2015-12-02},
  eprinttype = {arxiv},
  eprint     = {1511.08458 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/WKB48WVG/O'Shea and Nash - 2015 - An Introduction to Convolutional Neural Networks.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/N3RTJKIY/1511.html:text/html}
}

@misc{detone_superpoint_2018,
  title      = {{SuperPoint}: Self-Supervised Interest Point Detection and Description},
  url        = {http://arxiv.org/abs/1712.07629},
  doi        = {10.48550/arXiv.1712.07629},
  shorttitle = {{SuperPoint}},
  abstract   = {This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the {MS}-{COCO} generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on {HPatches} when compared to {LIFT}, {SIFT} and {ORB}.},
  number     = {{arXiv}:1712.07629},
  publisher  = {{arXiv}},
  author     = {{DeTone}, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  urldate    = {2025-04-25},
  date       = {2018-04-19},
  eprinttype = {arxiv},
  eprint     = {1712.07629 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/H7PVZVCE/DeTone et al. - 2018 - SuperPoint Self-Supervised Interest Point Detection and Description.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/2N6CUEDF/1712.html:text/html}
}


@misc{sarlin_superglue_2020,
  title      = {{SuperGlue}: Learning Feature Matching with Graph Neural Networks},
  url        = {http://arxiv.org/abs/1911.11763},
  doi        = {10.48550/arXiv.1911.11763},
  shorttitle = {{SuperGlue}},
  abstract   = {This paper introduces {SuperGlue}, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling {SuperGlue} to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. {SuperGlue} outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern {GPU} and can be readily integrated into modern {SfM} or {SLAM} systems. The code and trained weights are publicly available at https://github.com/magicleap/{SuperGluePretrainedNetwork}.},
  number     = {{arXiv}:1911.11763},
  publisher  = {{arXiv}},
  author     = {Sarlin, Paul-Edouard and {DeTone}, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  urldate    = {2025-04-25},
  date       = {2020-03-28},
  eprinttype = {arxiv},
  eprint     = {1911.11763 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/N3SM6TNN/Sarlin et al. - 2020 - SuperGlue Learning Feature Matching with Graph Neural Networks.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/AI4YQG66/1911.html:text/html}
}


@article{kuang_semantic_2022,
  title        = {Semantic Terrain Segmentation in the Navigation Vision of Planetary Rovers—A Systematic Literature Review},
  volume       = {22},
  rights       = {http://creativecommons.org/licenses/by/3.0/},
  issn         = {1424-8220},
  url          = {https://www.mdpi.com/1424-8220/22/21/8393},
  doi          = {10.3390/s22218393},
  abstract     = {Background: The planetary rover is an essential platform for planetary exploration. Visual semantic segmentation is significant in the localization, perception, and path planning of the rover autonomy. Recent advances in computer vision and artificial intelligence brought about new opportunities. A systematic literature review ({SLR}) can help analyze existing solutions, discover available data, and identify potential gaps. Methods: A rigorous {SLR} has been conducted, and papers are selected from three databases ({IEEE} Xplore, Web of Science, and Scopus) from the start of records to May 2022. The 320 candidate studies were found by searching with keywords and bool operators, and they address the semantic terrain segmentation in the navigation vision of planetary rovers. Finally, after four rounds of screening, 30 papers were included with robust inclusion and exclusion criteria as well as quality assessment. Results: 30 studies were included for the review, and sub-research areas include navigation (16 studies), geological analysis (7 studies), exploration efficiency (10 studies), and others (3 studies) (overlaps exist). Five distributions are extendedly depicted (time, study type, geographical location, publisher, and experimental setting), which analyzes the included study from the view of community interests, development status, and reimplementation ability. One key research question and six sub-research questions are discussed to evaluate the current achievements and future gaps. Conclusions: Many promising achievements in accuracy, available data, and real-time performance have been promoted by computer vision and artificial intelligence. However, a solution that satisfies pixel-level segmentation, real-time inference time, and onboard hardware does not exist, and an open, pixel-level annotated, and the real-world data-based dataset is not found. As planetary exploration projects progress worldwide, more promising studies will be proposed, and deep learning will bring more opportunities and contributions to future studies. Contributions: This {SLR} identifies future gaps and challenges by proposing a methodical, replicable, and transparent survey, which is the first review (also the first {SLR}) for semantic terrain segmentation in the navigation vision of planetary rovers.},
  pages        = {8393},
  number       = {21},
  journaltitle = {Sensors},
  author       = {Kuang, Boyu and Gu, Chengzhen and Rana, Zeeshan A. and Zhao, Yifan and Sun, Shuang and Nnabuife, Somtochukwu Godfrey},
  urldate      = {2025-10-04},
  date         = {2022-01},
  langid       = {english},
  note         = {Publisher: Multidisciplinary Digital Publishing Institute},
  keywords     = {image processing, open dataset, rover autonomy, terrain annotation, visual localization},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/WI8UK7VT/Kuang et al. - 2022 - Semantic Terrain Segmentation in the Navigation Vision of Planetary Rovers—A Systematic Literature R.pdf:application/pdf}
}

@article{jia_moon_2021,
  title        = {Moon Impact Crater Detection Using Nested Attention Mechanism Based {UNet}++},
  volume       = {9},
  issn         = {2169-3536},
  url          = {https://ieeexplore.ieee.org/abstract/document/9380415},
  doi          = {10.1109/ACCESS.2021.3066445},
  abstract     = {Impact craters are the most prominent topographic feature on the lunar surface, which will play a significant role in constructing lunar bases and lunar surface activities in the future. Traditional meteorite crater recognition methods are mainly based on artificial interpretation, usually combined with classical image processing methods. However, due to the different diameters and shapes of impact craters, the traditional crater identification methods have significant errors and low efficiency for small or overlapping impact craters. This paper proposes an automated algorithm termed nested attention-aware U-Net ({NAU}-Net) for crater detection using a lunar's digital elevation model ({DEM}). It then uses template matching to effectively calculate the longitude, latitude, and radius of the crater. It is worth mentioning that {NAU}-Net is primarily based on {UNet}++ and attention networks and applies a sequence of nested dense convolution blocks, preferably of classical convolution, which combines U-Net++ and Attention Gates advantages. Since our network uses nested intensive attentional aware connections and in-depth supervision, the training process of the {NAU}-Net is simple, which can achieve more accurate detection and recognition. In fact, the network can recognize smaller or overlapping impact craters and larger and more complex ones. In honor of lunar impact craters, compared with U-Net, {UNet}++, Dense-Unet, Attention-Unet, R2-Unet, our model achieved recall rates and accuracy of 0.791 and 0.856, better than other improved U-Net models. The experimental results show that the {NAU}-Net model can be used to extract impact craters.},
  pages        = {44107--44116},
  journaltitle = {{IEEE} Access},
  author       = {Jia, Yutong and Liu, Lei and Zhang, Chenyang},
  urldate      = {2025-10-04},
  date         = {2021},
  keywords     = {Convolution, Crater recognition, deep supervision, dense convolution block, Image segmentation, Logic gates, Moon, nested attention-aware U-Net, Semantics, Surface morphology, template matching, Transforms},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/SDVLZR29/Jia et al. - 2021 - Moon Impact Crater Detection Using Nested Attention Mechanism Based UNet++.pdf:application/pdf}
}


@misc{bochkovskii_depth_2025,
  title      = {Depth Pro: Sharp Monocular Metric Depth in Less Than a Second},
  url        = {http://arxiv.org/abs/2410.02073},
  doi        = {10.48550/arXiv.2410.02073},
  shorttitle = {Depth Pro},
  abstract   = {We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard {GPU}. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions. We release code and weights at https://github.com/apple/ml-depth-pro},
  number     = {{arXiv}:2410.02073},
  publisher  = {{arXiv}},
  author     = {Bochkovskii, Aleksei and Delaunoy, Amaël and Germain, Hugo and Santos, Marcel and Zhou, Yichao and Richter, Stephan R. and Koltun, Vladlen},
  urldate    = {2025-04-25},
  date       = {2025-04-21},
  eprinttype = {arxiv},
  eprint     = {2410.02073 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/KA56E8HC/Bochkovskii et al. - 2025 - Depth Pro Sharp Monocular Metric Depth in Less Than a Second.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/LR8MWRK3/2410.html:text/html}
}

@inproceedings{lipson_raft-stereo_2021,
  title      = {{RAFT}-Stereo: Multilevel Recurrent Field Transforms for Stereo Matching},
  url        = {https://ieeexplore.ieee.org/abstract/document/9665883},
  doi        = {10.1109/3DV53792.2021.00032},
  shorttitle = {{RAFT}-Stereo},
  abstract   = {We introduce {RAFT}-Stereo, a new deep architecture for rectified stereo based on the optical flow network {RAFT} [35]. We introduce multi-level convolutional {GRUs}, which more efficiently propagate information across the image. A modified version of {RAFT}-Stereo can perform accurate real-time inference. {RAFT}-stereo ranks first on the Middlebury leaderboard, outperforming the next best method on 1px error by 29\% and outperforms all published work on the {ETH}3D two-view stereo benchmark. Code is available at https://github.com/princeton-vl/{RAFT}-Stereo.},
  eventtitle = {2021 International Conference on 3D Vision (3DV)},
  pages      = {218--227},
  booktitle  = {2021 International Conference on 3D Vision (3DV)},
  author     = {Lipson, Lahav and Teed, Zachary and Deng, Jia},
  urldate    = {2025-04-25},
  date       = {2021-12},
  note       = {{ISSN}: 2475-7888},
  keywords   = {Real-time systems, Benchmark testing, Three-dimensional displays, Matching, Transforms, Optical flow, Convolutional codes, Deep, Deep architecture, {GRU}, Recurrent, Stereo},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/HC6K5ZKH/Lipson et al. - 2021 - RAFT-Stereo Multilevel Recurrent Field Transforms for Stereo Matching.pdf:application/pdf}
}

@online{liu_lusnarlunar_2024,
  title      = {{LuSNAR}:A Lunar Segmentation, Navigation and Reconstruction Dataset based on Muti-sensor for Autonomous Exploration},
  url        = {https://arxiv.org/abs/2407.06512v3},
  shorttitle = {{LuSNAR}},
  abstract   = {With the complexity of lunar exploration missions, the moon needs to have a higher level of autonomy. Environmental perception and navigation algorithms are the foundation for lunar rovers to achieve autonomous exploration. The development and verification of algorithms require highly reliable data support. Most of the existing lunar datasets are targeted at a single task, lacking diverse scenes and high-precision ground truth labels. To address this issue, we propose a multi-task, multi-scene, and multi-label lunar benchmark dataset {LuSNAR}. This dataset can be used for comprehensive evaluation of autonomous perception and navigation systems, including high-resolution stereo image pairs, panoramic semantic labels, dense depth maps, {LiDAR} point clouds, and the position of rover. In order to provide richer scene data, we built 9 lunar simulation scenes based on Unreal Engine. Each scene is divided according to topographic relief and the density of objects. To verify the usability of the dataset, we evaluated and analyzed the algorithms of semantic segmentation, 3D reconstruction, and autonomous navigation. The experiment results prove that the dataset proposed in this paper can be used for ground verification of tasks such as autonomous environment perception and navigation, and provides a lunar benchmark dataset for testing the accessibility of algorithm metrics. We make {LuSNAR} publicly available at: https://github.com/zqyu9/{LuSNAR}-dataset.},
  titleaddon = {{arXiv}.org},
  author     = {Liu, Jiayi and Zhang, Qianyu and Wan, Xue and Zhang, Shengyang and Tian, Yaolin and Han, Haodong and Zhao, Yutao and Liu, Baichuan and Zhao, Zeyuan and Luo, Xubo},
  urldate    = {2025-06-03},
  date       = {2024-07-09},
  langid     = {english},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/BGH2L2R2/Liu et al. - 2024 - LuSNARA Lunar Segmentation, Navigation and Reconstruction Dataset based on Muti-sensor for Autonomo.pdf:application/pdf}
}

@online{jhuapl_lunar_nodate,
  title      = {Lunar Autonomy Challenge ({LAC})},
  url        = {https://lunar-autonomy-challenge.jhuapl.edu/},
  abstract   = {The Lunar Autonomy Challenge invites teams at U.S. colleges and universities to virtually explore and map the lunar surface using a digital twin of {NASA}’s lunar mobility robot, the {ISRU} Pilot Excavator ({IPEx}). Teams will develop software that can perform set actions without human intervention, navigating the digital {IPEx} in the harsh, low-light conditions of the Moon.},
  titleaddon = {Lunar Autonomy Challenge ({LAC})},
  author     = {{JHUAPL}},
  urldate    = {2025-06-05},
  file       = {Snapshot:/Users/guillemcv/AppData/Zotero/storage/8MZW9QK9/lunar-autonomy-challenge.jhuapl.edu.html:text/html}
}

@online{noauthor_top_2025,
  title    = {Top Prize Awarded in Lunar Autonomy Challenge},
  url      = {https://www.nasa.gov/directorates/stmd/top-prize-awarded-in-lunar-autonomy-challenge-to-virtually-map-moons-surface/},
  abstract = {{NASA} named Stanford University of California winner of the Lunar Autonomy Challenge, a six-month competition...},
  urldate  = {2025-06-05},
  date     = {2025-05-15},
  langid   = {american},
  note     = {Section: Space Technology Mission Directorate},
  file     = {Snapshot:/Users/guillemcv/AppData/Zotero/storage/4ZYEYTIY/top-prize-awarded-in-lunar-autonomy-challenge-to-virtually-map-moons-surface.html:text/html}
}

@misc{ronneberger_u-net_2015,
  title      = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  url        = {http://arxiv.org/abs/1505.04597},
  doi        = {10.48550/arXiv.1505.04597},
  shorttitle = {U-Net},
  abstract   = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the {ISBI} challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and {DIC}) we won the {ISBI} cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent {GPU}. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  number     = {{arXiv}:1505.04597},
  publisher  = {{arXiv}},
  author     = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  urldate    = {2025-06-05},
  date       = {2015-05-18},
  eprinttype = {arxiv},
  eprint     = {1505.04597 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/3E3JZNRI/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/DX26754X/1505.html:text/html}
}

@misc{zhou_unet_2018,
  title      = {{UNet}++: A Nested U-Net Architecture for Medical Image Segmentation},
  url        = {http://arxiv.org/abs/1807.10165},
  doi        = {10.48550/arXiv.1807.10165},
  shorttitle = {{UNet}++},
  abstract   = {In this paper, we present {UNet}++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated {UNet}++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose {CT} scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal {CT} scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that {UNet}++ with deep supervision achieves an average {IoU} gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  number     = {{arXiv}:1807.10165},
  publisher  = {{arXiv}},
  author     = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  urldate    = {2025-06-05},
  date       = {2018-07-18},
  eprinttype = {arxiv},
  eprint     = {1807.10165 [cs]},
  keywords   = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/23K7CIWI/Zhou et al. - 2018 - UNet++ A Nested U-Net Architecture for Medical Image Segmentation.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/UNEND7X2/1807.html:text/html}
}

@article{fan_ma-net_2020,
  title        = {{MA}-Net: A Multi-Scale Attention Network for Liver and Tumor Segmentation},
  volume       = {8},
  issn         = {2169-3536},
  url          = {https://ieeexplore.ieee.org/abstract/document/9201310},
  doi          = {10.1109/ACCESS.2020.3025372},
  shorttitle   = {{MA}-Net},
  abstract     = {Automatic assessing the location and extent of liver and liver tumor is critical for radiologists, diagnosis and the clinical process. In recent years, a large number of variants of U-Net based on Multi-scale feature fusion are proposed to improve the segmentation performance for medical image segmentation. Unlike the previous works which extract the context information of medical image via applying the multi-scale feature fusion, we propose a novel network named Multi-scale Attention Net ({MA}-Net) by introducing self-attention mechanism into our method to adaptively integrate local features with their global dependencies. The {MA}-Net can capture rich contextual dependencies based on the attention mechanism. We design two blocks: Position-wise Attention Block ({PAB}) and Multi-scale Fusion Attention Block ({MFAB}). The {PAB} is used to model the feature interdependencies in spatial dimensions, which capture the spatial dependencies between pixels in a global view. In addition, the {MFAB} is to capture the channel dependencies between any feature map by multi-scale semantic feature fusion. We evaluate our method on the dataset of {MICCAI} 2017 {LiTS} Challenge. The proposed method achieves better performance than other state-of-the-art methods. The Dice values of liver and tumors segmentation are 0.960 ± 0.03 and 0.749 ± 0.08 respectively.},
  pages        = {179656--179665},
  journaltitle = {{IEEE} Access},
  author       = {Fan, Tongle and Wang, Guanglei and Li, Yan and Wang, Hongrui},
  urldate      = {2025-06-05},
  date         = {2020},
  keywords     = {deep learning, Feature extraction, Two dimensional displays, Semantics, attention mechanism, Biomedical imaging, context information, {CT}, Image segmentation, Liver, liver tumor segmentation, Tumors},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/QZA9BAHC/Fan et al. - 2020 - MA-Net A Multi-Scale Attention Network for Liver and Tumor Segmentation.pdf:application/pdf}
}

@inproceedings{chaurasia_linknet_2017,
  title      = {{LinkNet}: Exploiting Encoder Representations for Efficient Semantic Segmentation},
  url        = {http://arxiv.org/abs/1707.03718},
  doi        = {10.1109/VCIP.2017.8305148},
  shorttitle = {{LinkNet}},
  abstract   = {Pixel-wise semantic segmentation for visual scene understanding not only needs to be accurate, but also efficient in order to find any use in real-time application. Existing algorithms even though are accurate but they do not focus on utilizing the parameters of neural network efficiently. As a result they are huge in terms of parameters and number of operations; hence slow too. In this paper, we propose a novel deep neural network architecture which allows it to learn without any significant increase in number of parameters. Our network uses only 11.5 million parameters and 21.2 {GFLOPs} for processing an image of resolution 3x640x360. It gives state-of-the-art performance on {CamVid} and comparable results on Cityscapes dataset. We also compare our networks processing time on {NVIDIA} {GPU} and embedded system device with existing state-of-the-art architectures for different image resolutions.},
  pages      = {1--4},
  booktitle  = {2017 {IEEE} Visual Communications and Image Processing ({VCIP})},
  author     = {Chaurasia, Abhishek and Culurciello, Eugenio},
  urldate    = {2025-06-05},
  date       = {2017-12},
  eprinttype = {arxiv},
  eprint     = {1707.03718 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/QSZVQ9CN/Chaurasia and Culurciello - 2017 - LinkNet Exploiting Encoder Representations for Efficient Semantic Segmentation.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/CTDC846B/1707.html:text/html}
}

@article{kirillov_unified_nodate,
  title  = {A Unified Architecture for Instance and Semantic Segmentation},
  author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Dollár, Piotr},
  langid = {english},
  file   = {PDF:/Users/guillemcv/AppData/Zotero/storage/6NMSWLF3/Kirillov et al. - A Unified Architecture for Instance and Semantic Segmentation.pdf:application/pdf}
}

@misc{zhao_pyramid_2017,
  title      = {Pyramid Scene Parsing Network},
  url        = {http://arxiv.org/abs/1612.01105},
  doi        = {10.48550/arXiv.1612.01105},
  abstract   = {Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network ({PSPNet}). Our global prior representation is effective to produce good quality results on the scene parsing task, while {PSPNet} provides a superior framework for pixel-level prediction tasks. The proposed approach achieves state-of-the-art performance on various datasets. It came first in {ImageNet} scene parsing challenge 2016, {PASCAL} {VOC} 2012 benchmark and Cityscapes benchmark. A single {PSPNet} yields new record of {mIoU} accuracy 85.4\% on {PASCAL} {VOC} 2012 and accuracy 80.2\% on Cityscapes.},
  number     = {{arXiv}:1612.01105},
  publisher  = {{arXiv}},
  author     = {Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  urldate    = {2025-06-05},
  date       = {2017-04-27},
  eprinttype = {arxiv},
  eprint     = {1612.01105 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/44WM7ERF/Zhao et al. - 2017 - Pyramid Scene Parsing Network.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/WT8N7SGG/1612.html:text/html}
}

@misc{li_pyramid_2018,
  title      = {Pyramid Attention Network for Semantic Segmentation},
  url        = {http://arxiv.org/abs/1805.10180},
  doi        = {10.48550/arXiv.1805.10180},
  abstract   = {A Pyramid Attention Network({PAN}) is proposed to exploit the impact of global contextual information in semantic segmentation. Different from most existing works, we combine attention mechanism and spatial pyramid to extract precise dense features for pixel labeling instead of complicated dilated convolution and artificially designed decoder networks. Specifically, we introduce a Feature Pyramid Attention module to perform spatial pyramid attention structure on high-level output and combining global pooling to learn a better feature representation, and a Global Attention Upsample module on each decoder layer to provide global context as a guidance of low-level features to select category localization details. The proposed approach achieves state-of-the-art performance on {PASCAL} {VOC} 2012 and Cityscapes benchmarks with a new record of {mIoU} accuracy 84.0\% on {PASCAL} {VOC} 2012, while training without {COCO} dataset.},
  number     = {{arXiv}:1805.10180},
  publisher  = {{arXiv}},
  author     = {Li, Hanchao and Xiong, Pengfei and An, Jie and Wang, Lingxue},
  urldate    = {2025-06-05},
  date       = {2018-11-25},
  eprinttype = {arxiv},
  eprint     = {1805.10180 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/GBJE5JKG/Li et al. - 2018 - Pyramid Attention Network for Semantic Segmentation.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/LVLPLTPR/1805.html:text/html}
}

@misc{chen_rethinking_2017,
  title      = {Rethinking Atrous Convolution for Semantic Image Segmentation},
  url        = {http://arxiv.org/abs/1706.05587},
  doi        = {10.48550/arXiv.1706.05587},
  abstract   = {In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `{DeepLabv}3' system significantly improves over our previous {DeepLab} versions without {DenseCRF} post-processing and attains comparable performance with other state-of-art models on the {PASCAL} {VOC} 2012 semantic image segmentation benchmark.},
  number     = {{arXiv}:1706.05587},
  publisher  = {{arXiv}},
  author     = {Chen, Liang-Chieh and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  urldate    = {2025-06-05},
  date       = {2017-12-05},
  eprinttype = {arxiv},
  eprint     = {1706.05587 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/75LHE74L/Chen et al. - 2017 - Rethinking Atrous Convolution for Semantic Image Segmentation.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/FBDEXEBM/1706.html:text/html}
}

@misc{chen_encoder-decoder_2018,
  title      = {Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation},
  url        = {http://arxiv.org/abs/1802.02611},
  doi        = {10.48550/arXiv.1802.02611},
  abstract   = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, {DeepLabv}3+, extends {DeepLabv}3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on {PASCAL} {VOC} 2012 and Cityscapes datasets, achieving the test set performance of 89.0{\textbackslash}\% and 82.1{\textbackslash}\% without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at {\textbackslash}url\{https://github.com/tensorflow/models/tree/master/research/deeplab\}.},
  number     = {{arXiv}:1802.02611},
  publisher  = {{arXiv}},
  author     = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  urldate    = {2025-06-05},
  date       = {2018-08-22},
  eprinttype = {arxiv},
  eprint     = {1802.02611 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/H6IKX92I/Chen et al. - 2018 - Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/G9C39I8A/1802.html:text/html}
}

@misc{xiao_unified_2018,
  title      = {Unified Perceptual Parsing for Scene Understanding},
  url        = {http://arxiv.org/abs/1807.10221},
  doi        = {10.48550/arXiv.1807.10221},
  abstract   = {Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called {UPerNet} and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes. Models are available at {\textbackslash}url\{https://github.com/{CSAILVision}/unifiedparsing\}.},
  number     = {{arXiv}:1807.10221},
  publisher  = {{arXiv}},
  author     = {Xiao, Tete and Liu, Yingcheng and Zhou, Bolei and Jiang, Yuning and Sun, Jian},
  urldate    = {2025-06-05},
  date       = {2018-07-26},
  eprinttype = {arxiv},
  eprint     = {1807.10221 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/32FXMLLK/Xiao et al. - 2018 - Unified Perceptual Parsing for Scene Understanding.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/PSEYL73T/1807.html:text/html}
}

@misc{xie_segformer_2021,
  title      = {{SegFormer}: Simple and Efficient Design for Semantic Segmentation with Transformers},
  url        = {http://arxiv.org/abs/2105.15203},
  doi        = {10.48550/arXiv.2105.15203},
  shorttitle = {{SegFormer}},
  abstract   = {We present {SegFormer}, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception ({MLP}) decoders. {SegFormer} has two appealing features: 1) {SegFormer} comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) {SegFormer} avoids complex decoders. The proposed {MLP} decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from {SegFormer}-B0 to {SegFormer}-B5, reaching significantly better performance and efficiency than previous counterparts. For example, {SegFormer}-B4 achieves 50.3\% {mIoU} on {ADE}20K with 64M parameters, being 5x smaller and 2.2\% better than the previous best method. Our best model, {SegFormer}-B5, achieves 84.0\% {mIoU} on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/{NVlabs}/{SegFormer}.},
  number     = {{arXiv}:2105.15203},
  publisher  = {{arXiv}},
  author     = {Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M. and Luo, Ping},
  urldate    = {2025-06-05},
  date       = {2021-10-28},
  eprinttype = {arxiv},
  eprint     = {2105.15203 [cs]},
  keywords   = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/R7PZLAND/Xie et al. - 2021 - SegFormer Simple and Efficient Design for Semantic Segmentation with Transformers.pdf:application/pdf}
}

@misc{ranftl_vision_2021,
  title      = {Vision Transformers for Dense Prediction},
  url        = {http://arxiv.org/abs/2103.13413},
  doi        = {10.48550/arXiv.2103.13413},
  abstract   = {We introduce dense vision transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense vision transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28\% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on {ADE}20K with 49.02\% {mIoU}. We further show that the architecture can be fine-tuned on smaller datasets such as {NYUv}2, {KITTI}, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/{DPT}.},
  number     = {{arXiv}:2103.13413},
  publisher  = {{arXiv}},
  author     = {Ranftl, René and Bochkovskiy, Alexey and Koltun, Vladlen},
  urldate    = {2025-06-05},
  date       = {2021-03-24},
  eprinttype = {arxiv},
  eprint     = {2103.13413 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{dalal_gaussian_2024,
  title        = {Gaussian Splatting: 3D Reconstruction and Novel View Synthesis: A Review},
  volume       = {12},
  rights       = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
  issn         = {2169-3536},
  url          = {https://ieeexplore.ieee.org/document/10545567/},
  doi          = {10.1109/ACCESS.2024.3408318},
  shorttitle   = {Gaussian Splatting},
  abstract     = {Image-based 3D reconstruction is a challenging task that involves inferring the 3D shape of an object or scene from a set of input images. Learning-based methods have gained attention for their ability to directly estimate 3D shapes. This review paper focuses on state-of-the-art techniques for 3D reconstruction, including the generation of novel, unseen views. An overview of recent developments in the Gaussian Splatting method is provided, covering input types, model structures, output representations, and training strategies. Unresolved challenges and future directions are also discussed. Given the rapid progress in this domain and the numerous opportunities for enhancing 3D reconstruction methods, a comprehensive examination of algorithms appears essential. Consequently, this study offers a thorough overview of the latest advancements in Gaussian Splatting.},
  pages        = {96797--96820},
  journaltitle = {{IEEE} Access},
  shortjournal = {{IEEE} Access},
  author       = {Dalal, Anurag and Hagen, Daniel and Robbersmyr, Kjell G. and Knausgård, Kristian Muri},
  urldate      = {2025-06-05},
  date         = {2024},
  langid       = {english},
  file         = {PDF:/Users/guillemcv/AppData/Zotero/storage/XSIRWTHG/Dalal et al. - 2024 - Gaussian Splatting 3D Reconstruction and Novel View Synthesis A Review.pdf:application/pdf}
}

@article{lahiri_deep_2024,
  title        = {Deep Learning-Based Stereopsis and Monocular Depth Estimation Techniques: A Review},
  volume       = {6},
  rights       = {https://creativecommons.org/licenses/by/4.0/},
  issn         = {2624-8921},
  url          = {https://www.mdpi.com/2624-8921/6/1/13},
  doi          = {10.3390/vehicles6010013},
  shorttitle   = {Deep Learning-Based Stereopsis and Monocular Depth Estimation Techniques},
  abstract     = {A lot of research has been conducted in recent years on stereo depth estimation techniques, taking the traditional approach to a new level such that it is in an appreciably good form for competing in the depth estimation market with other methods, despite its few demerits. Sufficient progress in accuracy and depth computation speed has manifested during the period. Over the years, stereo depth estimation has been provided with various training modes, such as supervised, self-supervised, and unsupervised, before deploying it for real-time performance. These modes are to be used depending on the application and/or the availability of datasets for training. Deep learning, on the other hand, has provided the stereo depth estimation methods with a new life to breathe in the form of enhanced accuracy and quality of images, attempting to successfully reduce the residual errors in stages in some of the methods. Furthermore, depth estimation from a single {RGB} image has been intricate since it is an ill-posed problem with a lack of geometric constraints and ambiguities. However, this monocular depth estimation has gained popularity in recent years due to the development in the field, with appreciable improvements in the accuracy of depth maps and optimization of computational time. The help is mostly due to the usage of {CNNs} (Convolutional Neural Networks) and other deep learning methods, which help augment the feature-extraction phenomenon for the process and enhance the quality of depth maps/accuracy of {MDE} (monocular depth estimation). Monocular depth estimation has seen improvements in many algorithms that can be deployed to give depth maps with better clarity and details around the edges and fine boundaries, which thus helps in delineating between thin structures. This paper reviews various recent deep learning-based stereo and monocular depth prediction techniques emphasizing the successes achieved so far, the challenges acquainted with them, and those that can be expected shortly.},
  pages        = {305--351},
  number       = {1},
  journaltitle = {Vehicles},
  shortjournal = {Vehicles},
  author       = {Lahiri, Somnath and Ren, Jing and Lin, Xianke},
  urldate      = {2025-06-08},
  date         = {2024-01-31},
  langid       = {english},
  file         = {PDF:/Users/guillemcv/AppData/Zotero/storage/2ZJTDFNV/Lahiri et al. - 2024 - Deep Learning-Based Stereopsis and Monocular Depth Estimation Techniques A Review.pdf:application/pdf}
}

@incollection{wang_eai-stereo_2023,
  location   = {Cham},
  title      = {{EAI}-Stereo: Error Aware Iterative Network for Stereo Matching},
  volume     = {13841},
  isbn       = {978-3-031-26318-7 978-3-031-26319-4},
  url        = {https://link.springer.com/10.1007/978-3-031-26319-4_1},
  shorttitle = {{EAI}-Stereo},
  abstract   = {Current state-of-the-art stereo algorithms use a 2D {CNN} to extract features and then form a cost volume, which is fed into the following cost aggregation and regularization module composed of 2D or 3D {CNNs}. However, a large amount of high-frequency information like texture, color variation, sharp edge etc. is not well exploited during this process, which leads to relatively blurry and lacking detailed disparity maps. In this paper, we aim at making full use of the high-frequency information from the original image. Towards this end, we propose an error-aware refinement module that incorporates high-frequency information from the original left image and allows the network to learn error correction capabilities that can produce excellent subtle details and sharp edges. In order to improve the data transfer efficiency between our iterations, we propose the Iterative Multiscale Wide-{LSTM} Network which could carry more semantic information across iterations. We demonstrate the efficiency and effectiveness of our method on {KITTI} 2015, Middlebury, and {ETH}3D. At the time of writing this paper, {EAI}-Stereo ranks 1st on the Middlebury leaderboard and 1st on the {ETH}3D Stereo benchmark for 50\% quantile metric and second for 0.5px error rate among all published methods. Our model performs well in cross-domain scenarios and outperforms current methods specifically designed for generalization. Code is available at https://github.com/David-Zhao-1997/{EAI}-Stereo.},
  pages      = {3--19},
  booktitle  = {Computer Vision – {ACCV} 2022},
  publisher  = {Springer Nature Switzerland},
  author     = {Zhao, Haoliang and Zhou, Huizhou and Zhang, Yongjun and Zhao, Yong and Yang, Yitong and Ouyang, Ting},
  editor     = {Wang, Lei and Gall, Juergen and Chin, Tat-Jun and Sato, Imari and Chellappa, Rama},
  urldate    = {2025-06-08},
  date       = {2023},
  langid     = {english},
  doi        = {10.1007/978-3-031-26319-4_1},
  note       = {Series Title: Lecture Notes in Computer Science},
  file       = {PDF:/Users/guillemcv/AppData/Zotero/storage/FL5I4SN4/Zhao et al. - 2023 - EAI-Stereo Error Aware Iterative Network for Stereo Matching.pdf:application/pdf}
}

@inproceedings{detone_superpoint_2018-1,
  location   = {Salt Lake City, {UT}, {USA}},
  title      = {{SuperPoint}: Self-Supervised Interest Point Detection and Description},
  isbn       = {978-1-5386-6100-0},
  url        = {https://ieeexplore.ieee.org/document/8575521/},
  doi        = {10.1109/CVPRW.2018.00060},
  shorttitle = {{SuperPoint}},
  abstract   = {This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multihomography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the {MS}-{COCO} generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The ﬁnal system gives rise to state-of-the-art homography estimation results on {HPatches} when compared to {LIFT}, {SIFT} and {ORB}.},
  eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
  pages      = {337--33712},
  booktitle  = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
  publisher  = {{IEEE}},
  author     = {{DeTone}, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  urldate    = {2025-06-09},
  date       = {2018-06},
  langid     = {english},
  file       = {PDF:/Users/guillemcv/AppData/Zotero/storage/FUK2PAJL/DeTone et al. - 2018 - SuperPoint Self-Supervised Interest Point Detection and Description.pdf:application/pdf}
}

@inproceedings{sarlin_superglue_2020-1,
  location   = {Seattle, {WA}, {USA}},
  title      = {{SuperGlue}: Learning Feature Matching With Graph Neural Networks},
  rights     = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
  isbn       = {978-1-7281-7168-5},
  url        = {https://ieeexplore.ieee.org/document/9157489/},
  doi        = {10.1109/CVPR42600.2020.00499},
  shorttitle = {{SuperGlue}},
  abstract   = {This paper introduces {SuperGlue}, a neural network that matches two sets of local features by jointly ﬁnding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a ﬂexible context aggregation mechanism based on attention, enabling {SuperGlue} to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. {SuperGlue} outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern {GPU} and can be readily integrated into modern {SfM} or {SLAM} systems. The code and trained weights are publicly available at github.com/magicleap/{SuperGluePretrainedNetwork}.},
  eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  pages      = {4937--4946},
  booktitle  = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  publisher  = {{IEEE}},
  author     = {Sarlin, Paul-Edouard and {DeTone}, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew},
  urldate    = {2025-06-09},
  date       = {2020-06},
  langid     = {english},
  file       = {PDF:/Users/guillemcv/AppData/Zotero/storage/4D6SJ2A9/Sarlin et al. - 2020 - SuperGlue Learning Feature Matching With Graph Neural Networks.pdf:application/pdf}
}

@misc{li_practical_2022,
  title      = {Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation},
  url        = {http://arxiv.org/abs/2203.11483},
  doi        = {10.48550/arXiv.2203.11483},
  abstract   = {With the advent of convolutional neural networks, stereo matching algorithms have recently gained tremendous progress. However, it remains a great challenge to accurately extract disparities from real-world image pairs taken by consumer-level devices like smartphones, due to practical complicating factors such as thin structures, non-ideal rectification, camera module inconsistencies and various hard-case scenes. In this paper, we propose a set of innovative designs to tackle the problem of practical stereo matching: 1) to better recover fine depth details, we design a hierarchical network with recurrent refinement to update disparities in a coarse-to-fine manner, as well as a stacked cascaded architecture for inference; 2) we propose an adaptive group correlation layer to mitigate the impact of erroneous rectification; 3) we introduce a new synthetic dataset with special attention to difficult cases for better generalizing to real-world scenes. Our results not only rank 1st on both Middlebury and {ETH}3D benchmarks, outperforming existing state-of-the-art methods by a notable margin, but also exhibit high-quality details for real-life photos, which clearly demonstrates the efficacy of our contributions.},
  number     = {{arXiv}:2203.11483},
  publisher  = {{arXiv}},
  author     = {Li, Jiankun and Wang, Peisen and Xiong, Pengfei and Cai, Tao and Yan, Ziwei and Yang, Lei and Liu, Jiangyu and Fan, Haoqiang and Liu, Shuaicheng},
  urldate    = {2025-06-09},
  date       = {2022-03-22},
  eprinttype = {arxiv},
  eprint     = {2203.11483 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/ABQ7PE8K/Li et al. - 2022 - Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/DPWDBV8F/2203.html:text/html}
}

@article{hirschmuller_stereo_2008,
  title        = {Stereo Processing by Semiglobal Matching and Mutual Information},
  volume       = {30},
  issn         = {1939-3539},
  url          = {https://ieeexplore.ieee.org/document/4359315},
  doi          = {10.1109/TPAMI.2007.1166},
  abstract     = {This paper describes the semiglobal matching ({SGM}) stereo method. It uses a pixelwise, mutual information (Ml)-based matching cost for compensating radiometric differences of input images. Pixelwise matching is supported by a smoothness constraint that is usually expressed as a global cost function. {SGM} performs a fast approximation by pathwise optimizations from all directions. The discussion also addresses occlusion detection, subpixel refinement, and multibaseline matching. Additionally, postprocessing steps for removing outliers, recovering from specific problems of structured environments, and the interpolation of gaps are presented. Finally, strategies for processing almost arbitrarily large images and fusion of disparity images using orthographic projection are proposed. A comparison on standard stereo images shows that {SGM} is among the currently top-ranked algorithms and is best, if subpixel accuracy is considered. The complexity is linear to the number of pixels and disparity range, which results in a runtime of just 1-2 seconds on typical test images. An in depth evaluation of the Ml-based matching cost demonstrates a tolerance against a wide range of radiometric transformations. Finally, examples of reconstructions from huge aerial frame and pushbroom images demonstrate that the presented ideas are working well on practical problems.},
  pages        = {328--341},
  number       = {2},
  journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  author       = {Hirschmuller, Heiko},
  urldate      = {2025-06-09},
  date         = {2008-02},
  keywords     = {Computational efficiency, Cost function, global optimization, Image reconstruction, Image segmentation, Interpolation, multi-baseline, mutual information, Mutual information, Pixel, Radiometry, Runtime, stereo, Stereo vision},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/YMWPTCCL/Hirschmuller - 2008 - Stereo Processing by Semiglobal Matching and Mutual Information.pdf:application/pdf}
}


@inproceedings{wolf_gs2mesh_2025,
  location   = {Cham},
  title      = {{GS}2Mesh: Surface Reconstruction from Gaussian Splatting via Novel Stereo Views},
  isbn       = {978-3-031-73024-5},
  doi        = {10.1007/978-3-031-73024-5_13},
  shorttitle = {{GS}2Mesh},
  abstract   = {Recently, 3D Gaussian Splatting (3DGS) has emerged as an efficient approach for accurately representing scenes. However, despite its superior novel view synthesis capabilities, extracting the geometry of the scene directly from the Gaussian properties remains a challenge, as those are optimized based on a photometric loss. While some concurrent models have tried adding geometric constraints during the Gaussian optimization process, they still produce noisy, unrealistic surfaces.},
  pages      = {207--224},
  booktitle  = {Computer Vision – {ECCV} 2024},
  publisher  = {Springer Nature Switzerland},
  author     = {Wolf, Yaniv and Bracha, Amit and Kimmel, Ron},
  editor     = {Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
  date       = {2025},
  langid     = {english}
}


@article{bagdasarian_3dgszip_2025,
  title        = {3DGS.zip: A survey on 3D Gaussian Splatting Compression Methods},
  volume       = {44},
  rights       = {© 2025 The Author(s). Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley \& Sons Ltd.},
  issn         = {1467-8659},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.70078},
  doi          = {10.1111/cgf.70078},
  shorttitle   = {3DGS.zip},
  abstract     = {3D Gaussian Splatting (3DGS) has emerged as a cutting-edge technique for real-time radiance field rendering, offering state-of-the-art performance in terms of both quality and speed. 3DGS models a scene as a collection of three-dimensional Gaussians, with additional attributes optimized to conform to the scene's geometric and visual properties. Despite its advantages in rendering speed and image fidelity, 3DGS is limited by its significant storage and memory demands. These high demands make 3DGS impractical for mobile devices or headsets, reducing its applicability in important areas of computer graphics. To address these challenges and advance the practicality of 3DGS, this state-of-the-art report ({STAR}) provides a comprehensive and detailed examination of two complementary yet fundamentally distinct strategies: compression and compaction. Compression techniques focus on reducing the file size by encoding Gaussian attributes more efficiently. In contrast, compaction methods directly optimize the scene's structure by optimizing the number of Gaussian primitives. Notably, while methods in both categories aim to maintain or improve quality, each while minimizing its respective attributes—file size for compression and the number of Gaussians for compaction—compaction does not necessarily lead to smaller file sizes; it specifically targets improved efficiency during rendering, making it distinct from compression. We introduce the basic mathematical concepts underlying the analyzed methods, as well as key implementation details and design choices. Our report thoroughly discusses similarities and differences among the methods, as well as their respective advantages and disadvantages. We establish a consistent framework for comparing the surveyed methods based on key performance metrics and datasets. Specifically, since these methods have been developed in parallel and over a short period of time, currently, no comprehensive comparison exists. This survey, for the first time, presents a unified framework to evaluate 3DGS compression techniques. To facilitate the continuous monitoring of emerging methodologies, we maintain a dedicated website that will be regularly updated with new techniques and revisions of existing findings. Overall, this {STAR} provides an intuitive starting point for researchers interested in exploring the rapidly growing field of 3DGS compression. By comprehensively categorizing and evaluating existing compression and compaction strategies, our work advances the understanding and practical application of 3DGS in computationally constrained environments.},
  pages        = {e70078},
  number       = {2},
  journaltitle = {Computer Graphics Forum},
  author       = {Bagdasarian, M. T. and Knoll, P. and Li, Y. and Barthel, F. and Hilsmann, A. and Eisert, P. and Morgenstern, W.},
  urldate      = {2025-10-04},
  date         = {2025},
  langid       = {english},
  note         = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.70078},
  keywords     = {• Computing methodologies → Rasterization, • General and reference → Surveys and overviews, • Information systems → Data compression, {CCS} Concepts},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/ABMUBY7V/Bagdasarian et al. - 2025 - 3DGS.zip A survey on 3D Gaussian Splatting Compression Methods.pdf:application/pdf}
}


@article{yang_depth_2024,
  title        = {Depth Anything V2},
  volume       = {37},
  url          = {https://proceedings.neurips.cc/paper_files/paper/2024/hash/26cfdcd8fe6fd75cc53e92963a656c58-Abstract-Conference.html},
  pages        = {21875--21911},
  journaltitle = {Advances in Neural Information Processing Systems},
  author       = {Yang, Lihe and Kang, Bingyi and Huang, Zilong and Zhao, Zhen and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},
  urldate      = {2025-06-09},
  date         = {2024-12-16},
  langid       = {english},
  file         = {Full Text PDF:/Users/guillemcv/AppData/Zotero/storage/6VEFJFJ8/Yang et al. - 2024 - Depth Anything V2.pdf:application/pdf}
}

@online{nasa_apollo_2017,
  title  = {Apollo Lunar Surface Journal},
  author = {{NASA}},
  date   = {2017-12},
  file   = {Apollo Lunar Surface Journal:/Users/guillemcv/AppData/Zotero/storage/IH44ANS9/alsj.html:text/html}
}

@misc{kim_global-local_2022,
  title      = {Global-Local Path Networks for Monocular Depth Estimation with Vertical {CutDepth}},
  url        = {http://arxiv.org/abs/2201.07436},
  doi        = {10.48550/arXiv.2201.07436},
  abstract   = {Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset {NYU} Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalisation ability and robustness than other comparative models.},
  number     = {{arXiv}:2201.07436},
  publisher  = {{arXiv}},
  author     = {Kim, Doyeon and Ka, Woonghyun and Ahn, Pyungwhan and Joo, Donggyu and Chun, Sehwan and Kim, Junmo},
  urldate    = {2025-06-09},
  date       = {2022-10-29},
  eprinttype = {arxiv},
  eprint     = {2201.07436 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  file       = {Preprint PDF:/Users/guillemcv/AppData/Zotero/storage/4VBIZZ2T/Kim et al. - 2022 - Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth.pdf:application/pdf;Snapshot:/Users/guillemcv/AppData/Zotero/storage/ZL4JZKRB/2201.html:text/html}
}
